#import "../lib.typ": *
#import "../utils.typ": *
#import "@preview/splash:0.3.0": tailwind
#import "@preview/big-todo:0.2.0": *
= Related Works

#col[
  Traditionally, reverberation time (RT) estimation relied on the analysis of room impulse responses, which are often difficult to obtain in practical scenarios @kuttruffRoomAcoustics2006. This limitation has motivated the development of blind or non-intrusive methods that estimate RT directly from recorded audio, particularly speech.

  Early non-intrusive approaches exploited statistical properties of speech or energy decay characteristics. For example, maximum likelihood estimation and linear prediction methods were used to model the decay rate of the reverberant tail in speech @ratnamBlindEstimationReverberation2003. While effective under controlled conditions, these methods often struggle with background noise or highly non-stationary speech.

  With the rise of machine learning, more advanced techniques have been introduced. Several studies have combined handcrafted features — such as modulation spectra, spectral decay distributions, and energy-based features — with classical machine learning algorithms like support vector machines (SVMs) and random forests @lollmannImprovedAlgorithmBlind2010 @pregoBlindEstimatorsReverberation2015. These approaches have shown improved robustness and accuracy, especially when trained on diverse datasets representing a variety of acoustic environments.

  Recently, deep learning has gained traction for RT estimation. Deep neural networks, including convolutional and recurrent architectures, have been used to learn complex mappings from audio features to RT values @falconMachinelearningbasedEstimationReverberation2019 @pratesBlindEstimationReverberation. These data-driven methods have demonstrated strong performance, particularly in challenging acoustic conditions, but often require large labeled datasets and significant computational resources.

  The selection of input features is particularly important when estimating RT from images of a room. Recent work by #cite(<perezMachinelearningbasedEstimationReverberation>, form: "prose") has demonstrated approaches that utilize only a single room with rearranged furniture as input, relying on 360-degree panoramic images and 3D representations to capture spatial and structural changes reflected in these visual modalities. These methods leverage spatial cues from panoramic images or 3D room models to enhance the prediction of reverberation characteristics, as these formats provide a comprehensive view of the room's geometry and surface materials. This approach of using spatially rich features from a single physical space with varying furniture arrangements to improve estimation accuracy and robustness differs significantly from our methodology, which focuses on images captured from a single viewpoint from various rooms.

  Complementary to this, #cite(<milneUseArtificialIntelligence2020>, form: "prose") proposed a more comparable approach to ours by estimating RT using a set of $24$ RGB photographs per room, captured from different vertical angles, and only analyzing a single frequency band ($500 "Hz"$). Their results underscore the potential of image-based prediction even when relying solely on conventional RGB input. Likewise, the AV-RIR framework @ratnarajahAVRIRAudioVisualRoom2024 combined speech recordings with visual data to estimate room impulse responses. While this multimodal approach improves prediction precision, it also significantly increases data collection complexity and storage requirements.

  Additional studies further support the core idea that visual appearance encodes acoustically relevant information. In the work by #cite(<owensVisuallyIndicatedSounds2016>, form: "prose"), for example, a neural network was trained to predict the sounds produced when striking various materials, using only visual input from silent videos. This finding illustrates the potential of deep models to infer physical properties from images alone, even in the absence of explicit auditory data.

  Earlier work such as @kimAcousticRoomModelling2021 explored RT prediction using $360^degree$ stereo cameras. This approach effectively captured spatial and material information, though it required highly specialized equipment and non-standardized input formats. A more recent example is @chenVisualAcousticMatching2022, which proposed a model that transforms arbitrary audio signals to match the reverberation of the room depicted in a single image. While this work focuses on audio transformation rather than scalar RT prediction, it similarly relies on the acoustic cues embedded in RGB images, thereby aligning closely with our input design.

  Despite these advances, robust RT estimation across diverse real-world conditions remains challenging. Factors such as background noise, speaker variability, and multiple sound sources can degrade performance. Ongoing research continues to explore novel feature representations, improved learning architectures, and data augmentation strategies to enhance generalization and reliability @singhImage2ReverbCrossModalReverb2021.

  In summary, the field has progressed from statistical and signal processing-based methods to sophisticated machine learning and deep learning approaches, with increasing emphasis on perceptually motivated features. The integration of auditory-inspired features with modern machine learning, as explored by #cite(<perezMachinelearningbasedEstimationReverberation>, form: "prose") and #cite(<singhImage2ReverbCrossModalReverb2021>, form: "prose"), represents a promising direction for advancing non-intrusive reverberation time estimation from images.

  Taken together, these studies illustrate a growing consensus that visual modalities can be effectively leveraged to infer acoustic properties. These projects, while diverse in input formats and output targets, consistently demonstrate that visual features such as surface texture, spatial layout, and room function are predictive of acoustic behavior. This body of work provides strong evidence that RT estimation from a single RGB image is not only technically plausible but well-aligned with current research directions and advances in multimodal learning.

  Further supporting the feasibility of visually based RT estimation, #cite(<milneUseArtificialIntelligence2020>, form: "prose") conducted a user study involving $36$ participants, of which $24$ had a high-level understanding of room acoustics. When asked to estimate RT values from room photographs, these experts demonstrated that humans can make reasonably accurate inferences based on visual cues alone. Notably, the AI system developed in the same study achieved $90.90%$ accuracy at $500 "Hz"$ — outperforming human estimates by $28.98%$ — and exhibited a significantly narrower range of individual errors. These results indicate that if humans can infer reverberation time from images with some accuracy, it is reasonable to assume that AI models can learn to do the same, potentially with greater consistency and precision.
]
