@article{eyringREVERBERATIONTIMEDEAD1930,
  title = {{{REVERBERATION TIME IN}} “{{DEAD}}” {{ROOMS}}},
  author = {Eyring, Carl F.},
  date = {1930-01-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {1},
  pages = {217--241},
  issn = {0001-4966},
  doi = {10.1121/1.1915175},
  url = {https://doi.org/10.1121/1.1915175},
  urldate = {2025-07-16},
  issue = {2A}
}

@article{falconperezSphericalMapsAcoustic2021,
  title = {Spherical {{Maps}} of {{Acoustic Properties}} as {{Feature Vectors}} in {{Machine-Learning-Based Estimation}} of {{Acoustic Parameters}}},
  author = {Falcón Pérez, Ricardo and Götz, Georg and Pulkki, Ville},
  date = {2021-09-10},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {J. Audio Eng. Soc.},
  volume = {69},
  number = {9},
  pages = {632--643},
  issn = {15494950},
  doi = {10.17743/jaes.2021.0011},
  url = {https://www.aes.org/e-lib/browse.cfm?elib=21460},
  urldate = {2025-03-16},
  langid = {english}
}

@article{garcia-lazaroSensoryPerceptualDecisional2024,
  title = {Sensory and {{Perceptual Decisional Processes Underlying}} the {{Perception}} of {{Reverberant Auditory Environments}}},
  author = {García-Lázaro, Haydée G. and Teng, Santani},
  date = {2024-08-01},
  journaltitle = {eNeuro},
  shortjournal = {eNeuro},
  volume = {11},
  number = {8},
  eprint = {39122554},
  eprinttype = {pubmed},
  publisher = {Society for Neuroscience},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0122-24.2024},
  url = {https://www.eneuro.org/content/11/8/ENEURO.0122-24.2024},
  urldate = {2025-07-18},
  abstract = {Reverberation, a ubiquitous feature of real-world acoustic environments, exhibits statistical regularities that human listeners leverage to self-orient, facilitate auditory perception, and understand their environment. Despite the extensive research on sound source representation in the auditory system, it remains unclear how the brain represents real-world reverberant environments. Here, we characterized the neural response to reverberation of varying realism by applying multivariate pattern analysis to electroencephalographic (EEG) brain signals. Human listeners (12 males and 8 females) heard speech samples convolved with real-world and synthetic reverberant impulse responses and judged whether the speech samples were in a “real” or “fake” environment, focusing on the reverberant background rather than the properties of speech itself. Participants distinguished real from synthetic reverberation with ∼75\% accuracy; EEG decoding reveals a multistage decoding time course, with dissociable components early in the stimulus presentation and later in the perioffset stage. The early component predominantly occurred in temporal electrode clusters, while the later component was prominent in centroparietal clusters. These findings suggest distinct neural stages in perceiving natural acoustic environments, likely reflecting sensory encoding and higher-level perceptual decision-making processes. Overall, our findings provide evidence that reverberation, rather than being largely suppressed as a noise-like signal, carries relevant environmental information and gains representation along the auditory system. This understanding also offers various applications; it provides insights for including reverberation as a cue to aid navigation for blind and visually impaired people. It also helps to enhance realism perception in immersive virtual reality settings, gaming, music, and film production.},
  langid = {english},
  keywords = {auditory perception,EEG,MVPA,natural acoustic environments,reverberation}
}

@article{jambrosicReverberationTimeMeasuring2008,
  title = {Reverberation Time Measuring Methods},
  author = {Jambrosic, Kristian and Horvat, Marko and Domitrovic, Hrvoje},
  date = {2008-05-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {123},
  pages = {3617--3617},
  publisher = {Acoustical Society of America (ASA)},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.2934829},
  url = {https://pubs.aip.org/jasa/article/123/5_Supplement/3617/715908/Reverberation-time-measuring-methods},
  urldate = {2025-07-18},
  abstract = {In this paper different well-established methods of reverberation time measurement are compared. Furthermore, the results obtained using these methods are compared to the results provided by some additional methods which could serve as an in situ tool if, for any reason, the reverberation time measurements cannot be carried out using the standardized methods. The methods compared in this paper include the standardized methods (EN ISO 3382:2000), namely the impulse response measured with pink noise, exponential sweep, MLS, but also pistol shots of different calibers, balloon bursts, gated external pink noise, and the B\&K filtered burst method. In order to make the comparison, the measurements were performed in four acoustically very different spaces - a rather small and well-damped listening room, a much bigger damped listening room, a rather reverberant atrium, and a large and very reverberant shoebox-shaped room. The results were evaluated according to signal-to-noise ratio criterion as well. Special attention has been given to the influence of room modes on measurement results.},
  issue = {5\_Supplement},
  langid = {english}
}

@article{lubeckBinauralReproductionMicrophone2025,
  title = {Binaural {{Reproduction}} of {{Microphone Array Recordings With 2D Video}} in {{Mixed Reality}}},
  author = {Lübeck, Tim and Ben-Hur, Zamir and Lou Alon, David and Crukley, Jeffery},
  date = {2025-07-07},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {J. Audio Eng. Soc.},
  volume = {73},
  number = {7/8},
  pages = {461--470},
  publisher = {Audio Engineering Society},
  issn = {1549-4950},
  doi = {10.17743/jaes.2022.0213},
  url = {https://aes2.org/publications/elibrary-page/?id=22924},
  urldate = {2025-07-18},
  abstract = {Head-worn devices equipped with microphone arrays and cameras can be used to capture the experience from a user’s perspective and reproduce it in virtual, mixed, or augmented reality. A concept that has recently introduced is to present the video capture as a 2D video screen augmented into the real-world environment through a mixed reality headset. This study presents such a system for reproducing audio and video capture from glasses arrays as a video “augment” along with binaural audio. Results of an initial listening experiment are presented, evaluating different state-of-the-art methods for binaural rendering. A stereo rendering through virtual loudspeakers attached to the video “augment” is compared with head-locked and world-locked binaural syntheses based on a binaural beamforming approach. The results suggest that listeners rated beamforming-based reproduction higher than stereo rendering. World-locked rendering was not rated significantly better than the head-locked version.},
  langid = {english}
}

@article{perezMachinelearningbasedEstimationReverberation,
  title = {Machine-Learning-Based Estimation of Reverberation Time Using Room Geometry for Room Effect Rendering},
  author = {Pérez, Ricardo FALCÓN and Götz, Georg and Pulkki, Ville},
  abstract = {This work presents a machine-learning-based method to estimate the reverberation time of a virtual room for auralization purposes. The models take as input geometric features of the room and output the estimated reverberation time values as function of frequency. The proposed model is trained and evaluated using a novel dataset composed of real-world acoustical measurements of a single room with 832 different configurations of furniture and absorptive materials, for multiple loudspeaker positions. The method achieves a prediction accuracy of approximately 90\% for most frequency bands. Furthermore, when comparing against the Sabine and Eyring methods, the proposed approach exhibits a much higher accuracy, especially at low frequencies.},
  langid = {english}
}

@online{RoomImpulseResponse,
  title = {Room Impulse Response Simulation with Stochastic Ray Tracing - MATLAB \&amp; Simulink},
  url = {https://www.mathworks.com/help/audio/ug/room-impulse-response-simulation-with-stochastic-ray-tracing.html},
  urldate = {2025-07-16},
  abstract = {Use stochastic ray tracing to simulate the impulse response of a simple room.},
  langid = {ngerman}
}

@online{RoomImpulseResponsea,
  title = {Room Impulse Response Simulation with the Image-Source Method and HRTF Interpolation - MATLAB \&amp; Simulink},
  url = {https://www.mathworks.com/help/audio/ug/room-impulse-response-simulation-with-image-source-method-and-hrtf-interpolation.html},
  urldate = {2025-07-16},
  abstract = {Simulate the impulse response of a "shoebox" (cuboid) empty room.},
  langid = {ngerman}
}

@book{sabineCollectedPapersAcoustics1922,
  title = {Collected Papers on Acoustics},
  author = {Sabine, Wallace Clement},
  namea = {{University of California Libraries}},
  nameatype = {collaborator},
  date = {1922},
  publisher = {Cambridge : Harvard University Press},
  url = {http://archive.org/details/collectedpaperso00sabi},
  urldate = {2025-07-16},
  abstract = {ix, 279 p. 27 cm},
  langid = {english},
  pagetotal = {308},
  keywords = {Architectural acoustics}
}

@inbook{sabineReverberation1922,
  title = {Reverberation},
  booktitle = {Collected Papers on Acoustics},
  namea = {{University of California Libraries}},
  nameatype = {collaborator},
  date = {1922},
  pages = {3--68},
  publisher = {Cambridge : Harvard University Press},
  url = {http://archive.org/details/collectedpaperso00sabi},
  urldate = {2025-07-16},
  bookauthor = {Sabine, Wallace Clement},
  langid = {english},
  keywords = {Architectural acoustics}
}

@article{schaabDemonstratorAuralizationControl2017,
  title = {Demonstrator for the Auralization and Control of the Room Divergence Effect},
  author = {Schaab, M and Dobmeier, V and Werner, S and Klein, F},
  date = {2017},
  abstract = {The goal of binaural headphone reproduction is to synthesize a virtual room or to resynthesize the acoustics of a real room. Former research has shown, that the acoustical divergence between the room presented over headphones and the actual listening room can violate the expectations of the listener. In this case, the perceived quality of the synthesized room is degraded despite of a technical correct synthesis of the ear signals. This effect is called room divergence effect and is measured in a reduction of externalization of sound events. This publication describes a demonstrator which auralizes this effect. For this purpose a 5 channel loudspeaker setup is measured with a KEMAR artificial head in two rooms. Additionally three algorithms are implemented to calculate virtual rooms in between the measured rooms. By listening to the unmodified rooms measurements and their modifications differences in externalization are distinguishable. The influence of each algorithm on externalization in a divergent listening scenario is evaluated in a listening test with 14 participants.},
  langid = {english}
}

@article{selvarajuGradCAMVisualExplanations2016,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2016},
  doi = {10.48550/ARXIV.1610.02391},
  url = {https://arxiv.org/abs/1610.02391},
  urldate = {2025-05-23},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.}
}

@article{shinn-cunninghamBarbaraShinnCunningham2015,
  title = {Barbara {{Shinn-Cunningham}}},
  author = {Shinn-Cunningham, Barbara},
  date = {2015-06},
  journaltitle = {Current Biology},
  volume = {25},
  number = {11},
  pages = {R442-R444},
  publisher = {Elsevier BV},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2015.02.060},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982215002298},
  urldate = {2025-07-18},
  abstract = {One aspect of hearing that has received relatively little attention by traditional psychophysicists is how echoes and reverberation in everyday spaces affect perception. In the ordinary world, echoes and reverberation are ubiquitous and influence the signals reaching the listener, the processing of these signals by the brain, and the resulting perception of both sound sources and the environment. Many aspects of the signals reaching the ear are altered or "distorted" by echoes and reverberation, including spectral content, interaural differences, and temporal structure. As a result, echoes and reverberation could influence many aspects of perception, including spatial hearing in direction and distance, speech intelligibility, and spatial unmasking. This paper reviews results from a number of studies examining how the acoustics of ordinary rooms affect various aspects of the signals reaching a listener's ears as well as resulting perception. While the acoustic effects of reverberant energy are often pronounced, performance on most behavioral tasks is relatively robust to these effects. These perceptual results suggest that listeners may not simply be adept at ignoring the signal distortion caused by ordinary room acoustics, but may be adapted to deal with its presence. These results are important for designing truly immersive spatial auditory displays, because they demonstrate the importance of reverberant energy for achieving a realistic, immersive experience.},
  langid = {english}
}

@article{wittebolHybridRoomAcoustic2024,
  title = {A Hybrid Room Acoustic Modeling Approach Combining Image Source, Acoustic Diffusion Equation, and Time-Domain Discontinuous {{Galerkin}} Methods},
  author = {Wittebol, Wouter and Wang, Huiqing and Hornikx, Maarten and Calamia, Paul},
  date = {2024-07-05},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {223},
  pages = {110068},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2024.110068},
  url = {https://www.sciencedirect.com/science/article/pii/S0003682X24002196},
  urldate = {2025-07-16},
  abstract = {In this paper a hybrid model is introduced that constructs a broadband room impulse response using a geometrical (image source method) and a statistical method (acoustic diffusion equation) for the high-frequency range, supported by a wave-based method (time-domain discontinuous Galerkin method) for the low-frequency range. A crucial element concerns the construction of the high-frequency impulse response where a transition from a predominantly specular (image source) to a predominantly diffuse sound-field (diffusion equation) is required. To achieve this transition an analytical envelope is introduced. A key factor is the room-averaged scattering coefficient which accounts for all scattering behavior of the room and determines the speed of transition from a specular to a non-specular sound-field. To evaluate its performance, the model is compared to a broadband wave-based solver for two reference scenarios. The hybrid model shows promising results in terms of reverberation time (T20), center time (Ts) and bass-ratio (BR). Aspects such as the used geometrical complexity, the ‘room-averaged’ scattering coefficients, and other model simplifications and assumptions are discussed.},
  keywords = {Acoustic diffusion equation method,Hybrid,Image source method,Room acoustics,Time-domain discontinuous Galerkin method}
}

@book{zotterAmbisonicsPractical3D2019,
  title = {Ambisonics: {{A Practical 3D Audio Theory}} for {{Recording}}, {{Studio Production}}, {{Sound Reinforcement}}, and {{Virtual Reality}}},
  shorttitle = {Ambisonics},
  author = {Zotter, Franz and Frank, Matthias},
  date = {2019},
  series = {Springer {{Topics}} in {{Signal Processing}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  issn = {1866-2609, 1866-2617},
  doi = {10.1007/978-3-030-17207-7},
  url = {http://link.springer.com/10.1007/978-3-030-17207-7},
  urldate = {2025-07-18},
  isbn = {978-3-030-17206-0 978-3-030-17207-7},
  langid = {english},
  keywords = {Ambisonics Book,Binaural Listening,Imaginary Loudspeakers,Open Access,Playback Technology,Psychoacoustics,Scene-based Sound,Spherical Array Processing,Surround Sound Recording}
}
