@article{alhawitiAdvancesArtificialIntelligence2015,
  title = {Advances {{In Artificial Intelligence Using Speech Recognition}}},
  author = {Alhawiti, Khaled M.},
  date = {2015-06-03},
  publisher = {Zenodo},
  doi = {10.5281/ZENODO.1106879},
  url = {https://zenodo.org/record/1106879},
  urldate = {2025-07-27},
  abstract = {This research study aims to present a retrospective study about speech recognition systems and artificial intelligence. Speech recognition has become one of the widely used technologies, as it offers great opportunity to interact and communicate with automated machines. Precisely, it can be affirmed that speech recognition facilitates its users and helps them to perform their daily routine tasks, in a more convenient and effective manner. This research intends to present the illustration of recent technological advancements, which are associated with artificial intelligence. Recent researches have revealed the fact that speech recognition is found to be the utmost issue, which affects the decoding of speech. In order to overcome these issues, different statistical models were developed by the researchers. Some of the most prominent statistical models include acoustic model (AM), language model (LM), lexicon model, and hidden Markov models (HMM). The research will help in understanding all of these statistical models of speech recognition. Researchers have also formulated different decoding methods, which are being utilized for realistic decoding tasks and constrained artificial languages. These decoding methods include pattern recognition, acoustic phonetic, and artificial intelligence. It has been recognized that artificial intelligence is the most efficient and reliable methods, which are being used in speech recognition.},
  langid = {english},
  keywords = {acoustic phonetic,artificial intelligence,Hidden Markov Models (HMM),human machine performance.,Speech recognition,statistical models of speech recognition}
}

@article{allenImageMethodEfficiently1979,
  title = {Image Method for Efficiently Simulating Small-Room Acoustics},
  author = {Allen, Jont and Berkley, David},
  date = {1979-04-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {65},
  pages = {943--950},
  doi = {10.1121/1.382599},
  abstract = {Image methods are commonly used for the analysis of the acoustic properties of enclosures. In this paper we discuss the theoretical and practical use of image techniques for simulating, on a digital computer, the impulse response between two points in a small rectangular room. The resulting impulse response, when convolved with any desired input signal, such as speech, simulates room reverberation of the input signal. This technique is useful in signal processing or psychoacoustic studies. The entire process is carried out on a digital computer so that a wide range of room parameters can be studied with accurate control over the experimental conditions. A FORTRAN implementation of this model has been included.}
}

@article{andrewsDimensionReductionPCA,
  title = {Dimension {{Reduction PCA}}, {{tSNE}}, {{UMAP}}, {{Integration}}},
  author = {Andrews, Simon},
  langid = {english}
}

@article{ashqarImageBasedTomatoLeaves2019,
  title = {Image-{{Based Tomato Leaves Diseases Detection Using Deep Learning}}},
  author = {Ashqar, Belal and Abu-Naser, Samy},
  date = {2019-01-01},
  journaltitle = {International Journal of Engineering Research},
  shortjournal = {International Journal of Engineering Research},
  volume = {2},
  pages = {10--16},
  abstract = {Crop diseases are a key danger for food security, but their speedy identification still difficult in many portions of the world because of the lack of the essential infrastructure. The mixture of increasing worldwide smartphone dispersion and current advances in computer vision made conceivable by deep learning has cemented the way for smartphone-assisted disease identification. Using a public dataset of 9000 images of infected and healthy Tomato leaves collected under controlled conditions, we trained a deep convolutional neural network to identify 5 diseases. The trained model achieved an accuracy of 99.84\% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.}
}

@book{beranekConcertHallsOpera2010,
  title = {Concert {{Halls}} and Opera Houses: Music, Acoustics, and Architecture},
  shorttitle = {Concert {{Halls}} and Opera Houses},
  author = {Beranek, Leo L.},
  date = {2010},
  edition = {2., nd ed., softcover version of orig. hardcover ed. 2004},
  publisher = {Springer},
  location = {New York, NY},
  isbn = {978-1-4419-3038-5},
  langid = {english},
  pagetotal = {661}
}

@book{bilbaoNumericalSoundSynthesis2009,
  title = {Numerical Sound Synthesis: Finite Difference Schemes and Simulation in Musical Acoustics},
  shorttitle = {Numerical Sound Synthesis},
  author = {Bilbao, Stefan D.},
  date = {2009},
  publisher = {John Wiley \& Sons},
  location = {Chichester},
  doi = {10.1002/9780470749012},
  isbn = {978-0-470-74901-2 978-0-470-51046-9 978-0-470-74902-9},
  langid = {english},
  pagetotal = {1}
}

@article{borishExtensionImageModel1984,
  title = {Extension of the Image Model to Arbitrary Polyhedra},
  author = {Borish, Jeffrey},
  date = {1984-06-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {75},
  number = {6},
  pages = {1827--1836},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.390983},
  url = {https://pubs.aip.org/jasa/article/75/6/1827/769678/Extension-of-the-image-model-to-arbitrary},
  urldate = {2025-07-27},
  abstract = {In this paper, the image model is extended to arbitrary polyhedra with any number of sides. This generalization makes it possible to model real concert halls much more accurately. The image positions computed by the method can be reduced to the directional impulse response in order to create audible simulations of the concert hall. Also, the image model can provide insight into the fundamental acoustical properties of different concert hall geometries. For example, the extended image model demonstrates that rectangular halls have an advantage over fan-shaped halls with regard to spatial impression. We also show why the image model has fundamental advantages over another popular modeling technique, ray tracing.},
  langid = {english}
}

@online{caiTheoreticalFoundationsTSNE2022,
  title = {Theoretical {{Foundations}} of T-{{SNE}} for {{Visualizing High-Dimensional Clustered Data}}},
  author = {Cai, T. Tony and Ma, Rong},
  date = {2022-11-01},
  eprint = {2105.07536},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2105.07536},
  url = {http://arxiv.org/abs/2105.07536},
  urldate = {2025-07-27},
  abstract = {This paper investigates the theoretical foundations of the t-distributed stochastic neighbor embedding (t-SNE) algorithm, a popular nonlinear dimension reduction and data visualization method. A novel theoretical framework for the analysis of t-SNE based on the gradient descent approach is presented. For the early exaggeration stage of t-SNE, we show its asymptotic equivalence to power iterations based on the underlying graph Laplacian, characterize its limiting behavior, and uncover its deep connection to Laplacian spectral clustering, and fundamental principles including early stopping as implicit regularization. The results explain the intrinsic mechanism and the empirical benefits of such a computational strategy. For the embedding stage of t-SNE, we characterize the kinematics of the low-dimensional map throughout the iterations, and identify an amplification phase, featuring the intercluster repulsion and the expansive behavior of the low-dimensional map, and a stabilization phase. The general theory explains the fast convergence rate and the exceptional empirical performance of t-SNE for visualizing clustered data, brings forth interpretations of the t-SNE visualizations, and provides theoretical guidance for applying t-SNE and selecting its tuning parameters in various applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory}
}

@online{chenVisualAcousticMatching2022,
  title = {Visual {{Acoustic Matching}}},
  author = {Chen, Changan and Gao, Ruohan and Calamia, Paul and Grauman, Kristen},
  date = {2022-06-13},
  eprint = {2202.06875},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.06875},
  url = {http://arxiv.org/abs/2202.06875},
  urldate = {2025-07-26},
  abstract = {We introduce the visual acoustic matching task, in which an audio clip is transformed to sound like it was recorded in a target environment. Given an image of the target environment and a waveform for the source audio, the goal is to re-synthesize the audio to match the target room acoustics as suggested by its visible geometry and materials. To address this novel task, we propose a cross-modal transformer model that uses audio-visual attention to inject visual properties into the audio and generate realistic audio output. In addition, we devise a self-supervised training objective that can learn acoustic matching from in-the-wild Web videos, despite their lack of acoustically mismatched audio. We demonstrate that our approach successfully translates human speech to a variety of real-world environments depicted in images, outperforming both traditional acoustic matching and more heavily supervised baselines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{chuComparisonReverberationMeasurements1978,
  title = {Comparison of Reverberation Measurements Using {{Schroeder}}'s Impulse Method and Decay-Curve Averaging Method},
  author = {Chu, W. T.},
  date = {1978-05},
  journaltitle = {Journal of the Acoustical Society of America},
  volume = {63},
  number = {5},
  pages = {1444--1450},
  issn = {0001-4966},
  abstract = {Utilizing a digital acquisition system and minicomputer, two promising techniques for accurate determination of reverberation times have been studied in detail from the viewpoint of standard reverberation room tests. The first is Schroeder's "integrated impulse method," and special attention was given to the question of repeatability and the influence of signal-to-noise ratio on the successful application of the method. The second technique involves taking an ensemble average of a large number of logarithmic decay curves. It was found that, even for nonuniform decays, the average decay curves obtained by the second method compared well with those determined by the Integrated Impulse Method.},
  langid = {english},
  keywords = {absorption acoustique,Acoustics,Acoustique,chambre réverbérante,durée de réverbération,reverberation chambers,reverberation time,sound absorption}
}

@inreference{CoefficientDetermination2025,
  title = {Coefficient of Determination},
  booktitle = {Wikipedia},
  date = {2025-07-22T00:31:45Z},
  url = {https://en.wikipedia.org/w/index.php?title=Coefficient_of_determination&oldid=1301836687},
  urldate = {2025-07-22},
  abstract = {In statistics, the coefficient of determination, denoted R2 or r2 and pronounced "R squared", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s). It is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. There are several definitions of R2 that are only sometimes equivalent. In simple linear regression (which includes an intercept), r2 is simply the square of the sample correlation coefficient (r), between the observed outcomes and the observed predictor values. If additional regressors are included, R2 is the square of the coefficient of multiple correlation. In both such cases, the coefficient of determination normally ranges from 0 to 1. There are cases where R2 can yield negative values. This can arise when the predictions that are being compared to the corresponding outcomes have not been derived from a model-fitting procedure using those data. Even if a model-fitting procedure has been used, R2 may still be negative, for example when linear regression is conducted without including an intercept, or when a non-linear function is used to fit the data. In cases where negative values arise, the mean of the data provides a better fit to the outcomes than do the fitted function values, according to this particular criterion. The coefficient of determination can be more intuitively informative than MAE, MAPE, MSE, and RMSE in regression analysis evaluation, as the former can be expressed as a percentage, whereas the latter measures have arbitrary ranges. It also proved more robust for poor fits compared to SMAPE on certain test datasets. When evaluating the goodness-of-fit of simulated (Ypred) versus measured (Yobs) values, it is not appropriate to base this on the R2 of the linear regression (i.e., Yobs= m·Ypred + b). The R2 quantifies the degree of any linear correlation between Yobs and Ypred, while for the goodness-of-fit evaluation only one specific linear correlation should be taken into consideration: Yobs = 1·Ypred + 0 (i.e., the 1:1 line).},
  langid = {english},
  annotation = {Page Version ID: 1301836687}
}

@book{cremerPrinciplesApplicationsRoom1982,
  title = {Principles and applications of room acoustics},
  author = {Cremer, Lothar and Müller, Helmut A.},
  date = {1982},
  publisher = {{Applied Science ; Sole distributor in the USA and Canada, Elsevier Science Pub. Co}},
  location = {London ; New York : New York, NY, USA},
  isbn = {978-0-85334-113-0 978-0-85334-114-7},
  langid = {eng ger},
  pagetotal = {2},
  keywords = {Architectural acoustics}
}

@article{delsolardorregoStudyJustNoticeable2022,
  title = {A Study of the Just Noticeable Difference of Early Decay Time for Symphonic Halls},
  author = {Del Solar Dorrego, Fernando and Vigeant, Michelle C.},
  date = {2022-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J Acoust Soc Am},
  volume = {151},
  number = {1},
  eprint = {35105034},
  eprinttype = {pubmed},
  pages = {80},
  issn = {1520-8524},
  doi = {10.1121/10.0009167},
  abstract = {The just noticeable differences (JNDs) of room acoustic parameters are important for the design of concert halls and, in general, research of room acoustics. Precise knowledge of JNDs helps the concert hall designer in assessing the impact that changes in the geometry or materials of the hall will have on its perceived acoustics. When designing a concert hall, creating an appropriate feeling of reverberance for the audience is of prime importance. The early decay time (EDT) parameter has proved to be a better predictor of the perception of reverberance than the classical reverberation time (T30), but no studies have been conducted to specifically determine the EDT JND. In the present study, the EDT JND was investigated for broadband conditions and assessed for individual frequency ranges. A subjective study was conducted with 26 subjects with musical training, in which 21 were considered reliable. The participants listened to orchestral music convolved with measured spatial room impulse responses from three concert halls. The stimuli were auralized in an anechoic chamber using third-order Ambisonic reproduction. The obtained values show that the JNDs for the broadband conditions are lower than those for the individual frequency ranges. The EDT JND for the broadband conditions was found to be approximately 18\% of the EDT value.},
  langid = {english},
  keywords = {Acoustics,Auditory Perception,Differential Threshold,Humans,Music}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  url = {https://ieeexplore.ieee.org/document/5206848},
  urldate = {2025-07-31},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine}
}

@book{doelleEnvironmentalAcoustics1972,
  title = {Environmental Acoustics},
  author = {Doelle, Leslie L.},
  namea = {{Internet Archive}},
  nameatype = {collaborator},
  date = {1972},
  publisher = {New York, McGraw-Hill},
  url = {http://archive.org/details/environmentalaco0000doel},
  urldate = {2025-07-27},
  abstract = {x, 246 p. 28 cm; Includes bibliographies},
  isbn = {978-0-07-017342-2},
  langid = {english},
  pagetotal = {266},
  keywords = {Architectural acoustics}
}

@article{eyringReverberationTimeDead1930,
  title = {Reverberation {{Time}} in “{{Dead}}” {{Rooms}}},
  author = {Eyring, Carl F.},
  date = {1930-01-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {1},
  pages = {217--241},
  issn = {0001-4966},
  doi = {10.1121/1.1915175},
  url = {https://doi.org/10.1121/1.1915175},
  urldate = {2025-07-16},
  issue = {2A}
}

@book{falconMachinelearningbasedEstimationReverberation2019,
  title = {Machine-Learning-Based Estimation of Reverberation Time Using Room Geometry for Room Effect Rendering},
  author = {Falcon, Ricardo and Götz, Georg and Pulkki, Ville},
  date = {2019-09-13},
  abstract = {This work presents a machine-learning-based method to estimate the reverberation time of a virtual room for auralization purposes. The models take as input geometric features of the room and output the estimated reverberation time values as function of frequency. The proposed model is trained and evaluated using a novel dataset composed of real-world acoustical measurements of a single room with 832 different configurations of furniture and absorptive materials, for multiple loudspeaker positions. The method achieves a prediction accuracy of approximately 90\% for most frequency bands. Furthermore, when comparing against the Sabine and Eyring methods, the proposed approach exhibits a much higher accuracy, especially at low frequencies.}
}

@article{falconperezSphericalMapsAcoustic2021,
  title = {Spherical {{Maps}} of {{Acoustic Properties}} as {{Feature Vectors}} in {{Machine-Learning-Based Estimation}} of {{Acoustic Parameters}}},
  author = {Falcón Pérez, Ricardo and Götz, Georg and Pulkki, Ville},
  date = {2021-09-10},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {J. Audio Eng. Soc.},
  volume = {69},
  number = {9},
  pages = {632--643},
  issn = {15494950},
  doi = {10.17743/jaes.2021.0011},
  url = {https://www.aes.org/e-lib/browse.cfm?elib=21460},
  urldate = {2025-03-16},
  langid = {english}
}

@article{falkNonIntrusiveQualityIntelligibility2010,
  title = {A {{Non-Intrusive Quality}} and {{Intelligibility Measure}} of {{Reverberant}} and {{Dereverberated Speech}}},
  author = {Falk, Tiago H. and Zheng, Chenxi and Chan, Wai-Yip},
  date = {2010-09},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE Trans. Audio Speech Lang. Process.},
  volume = {18},
  number = {7},
  pages = {1766--1774},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/tasl.2010.2052247},
  url = {http://ieeexplore.ieee.org/document/5547575/},
  urldate = {2025-07-20},
  abstract = {A modulation spectral representation is investigated for non-intrusive quality and intelligibility measurement of reverberant and dereverberated speech. The representation is obtained by means of an auditory-inspired filterbank analysis of criticalband temporal envelopes of the speech signal. Modulation spectral insights are used to develop an adaptive measure termed speech to reverberation modulation energy ratio. Experimental results show the proposed measure outperforming three standard algorithms for tasks involving estimation of multiple dimensions of perceived coloration, as well as quality measurement and intelligibility estimation of reverberant and dereverberated speech.},
  langid = {english}
}

@online{foundationBlenderorgHomeBlender,
  title = {Blender.Org - {{Home}} of the {{Blender}} Project - {{Free}} and {{Open 3D Creation Software}}},
  author = {Foundation, Blender},
  url = {https://www.blender.org/},
  urldate = {2025-07-19},
  abstract = {The Freedom to Create},
  langid = {english},
  organization = {blender.org}
}

@article{garcia-lazaroSensoryPerceptualDecisional2024,
  title = {Sensory and {{Perceptual Decisional Processes Underlying}} the {{Perception}} of {{Reverberant Auditory Environments}}},
  author = {García-Lázaro, Haydée G. and Teng, Santani},
  date = {2024-08-01},
  journaltitle = {eNeuro},
  shortjournal = {eNeuro},
  volume = {11},
  number = {8},
  eprint = {39122554},
  eprinttype = {pubmed},
  publisher = {Society for Neuroscience},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0122-24.2024},
  url = {https://www.eneuro.org/content/11/8/ENEURO.0122-24.2024},
  urldate = {2025-07-18},
  abstract = {Reverberation, a ubiquitous feature of real-world acoustic environments, exhibits statistical regularities that human listeners leverage to self-orient, facilitate auditory perception, and understand their environment. Despite the extensive research on sound source representation in the auditory system, it remains unclear how the brain represents real-world reverberant environments. Here, we characterized the neural response to reverberation of varying realism by applying multivariate pattern analysis to electroencephalographic (EEG) brain signals. Human listeners (12 males and 8 females) heard speech samples convolved with real-world and synthetic reverberant impulse responses and judged whether the speech samples were in a “real” or “fake” environment, focusing on the reverberant background rather than the properties of speech itself. Participants distinguished real from synthetic reverberation with ∼75\% accuracy; EEG decoding reveals a multistage decoding time course, with dissociable components early in the stimulus presentation and later in the perioffset stage. The early component predominantly occurred in temporal electrode clusters, while the later component was prominent in centroparietal clusters. These findings suggest distinct neural stages in perceiving natural acoustic environments, likely reflecting sensory encoding and higher-level perceptual decision-making processes. Overall, our findings provide evidence that reverberation, rather than being largely suppressed as a noise-like signal, carries relevant environmental information and gains representation along the auditory system. This understanding also offers various applications; it provides insights for including reverberation as a cue to aid navigation for blind and visually impaired people. It also helps to enhance realism perception in immersive virtual reality settings, gaming, music, and film production.},
  langid = {english},
  keywords = {auditory perception,EEG,MVPA,natural acoustic environments,reverberation}
}

@book{geronHandsonMachineLearning2023,
  title = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}} and {{TensorFlow}}: Concepts, Tools, and Techniques to Build Intelligent Systems},
  shorttitle = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}} and {{TensorFlow}}},
  author = {Géron, Aurélien},
  date = {2023},
  edition = {Third edition},
  publisher = {O'Reilly Media, Inc},
  location = {Beijing},
  abstract = {"Through a recent series of breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This best-selling book uses concrete examples, minimal theory, and production-ready Python frameworks--scikit-learn, Keras, and TensorFlow--to help you gain an intuitive understanding of the concepts and tools for building intelligent systems. With this updated third edition, author Aurelien Geron explores a range of techniques, starting with simple linear regression and progressing to deep neural networks. Numerous code examples and exercises throughout the book help you apply what you've learned. Programming experience is all you need to get started"--},
  isbn = {978-1-0981-2597-4},
  pagetotal = {834},
  keywords = {Apprentissage automatique,artificial intelligence,Artificial intelligence,Intelligence artificielle,Machine learning,Python (Computer program language),Python (Langage de programmation),TensorFlow}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  isbn = {978-0-262-03561-3},
  pagetotal = {775},
  keywords = {Machine learning}
}

@article{hamiltonTutorialFinitedifferenceTimedomain2021,
  title = {Tutorial on Finite-Difference Time-Domain ({{FDTD}}) Methods for Room Acoustics Simulation},
  author = {Hamilton, Brian},
  date = {2021-04-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {149},
  pages = {A92-A93},
  doi = {10.1121/10.0004614},
  abstract = {No PDF available ABSTRACT Wave-based simulation models for acoustics have remained an active area of research and development for the past three decades. Wave-based methods aim to solve the 3D wave equation directly and therefore have large computational costs relative to conventional ray-based methods, which tend to simplify wave-diffraction effects. However, wave-based methods offer the potential of complete numerical solutions, including all wave-scattering and diffraction effects over the full audible bandwidth. Additionally, wave-based methods are highly parallelisable, making them amenable to parallel computing architectures such as graphics processing units, which can greatly cut down lengthy simulation times. This tutorial will give an introduction to wave-based methods for room acoustics with a focus on the finite-difference time-domain (FDTD) method. The basic concepts behind FDTD methods, along with practical implementation issues, will be discussed and illustrated with examples. The relationship of FDTD methods to other wave-based methods, along with differences to ray-based methods, will be explained. Also, the use of graphics processing units (GPUs) with freely available FDTD software will be discussed.}
}

@online{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.03385},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2025-07-31},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@standard{internationalorganizationforstandardizationISO3382120092009,
  title = {{{ISO}} 3382-1:2009 {{Acoustics}} — {{Measurement}} of Room Acoustic Parameters},
  author = {{International Organization for Standardization}},
  date = {2009},
  number = {Part 1: Performance spaces},
  url = {https://www.iso.org/standard/40979.html}
}

@online{ISO338212009,
  title = {{{ISO}} 3382-1:2009},
  shorttitle = {{{ISO}} 3382-1},
  url = {https://www.iso.org/standard/40979.html},
  urldate = {2025-07-29},
  abstract = {Acoustics — Measurement of room acoustic parameters — Part 1: Performance spaces},
  langid = {english},
  organization = {ISO}
}

@standard{ISO3382220082008,
  title = {{{ISO}} 3382-2:2008 {{Acoustics}} — {{Measurement}} of Room Acoustic Parameters},
  date = {2008},
  number = {Part 2: Reverberation time in ordinary rooms}
}

@article{jambrosicReverberationTimeMeasuring2008,
  title = {Reverberation Time Measuring Methods},
  author = {Jambrosic, Kristian and Horvat, Marko and Domitrovic, Hrvoje},
  date = {2008-05-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {123},
  pages = {3617--3617},
  publisher = {Acoustical Society of America (ASA)},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.2934829},
  url = {https://pubs.aip.org/jasa/article/123/5_Supplement/3617/715908/Reverberation-time-measuring-methods},
  urldate = {2025-07-18},
  abstract = {In this paper different well-established methods of reverberation time measurement are compared. Furthermore, the results obtained using these methods are compared to the results provided by some additional methods which could serve as an in situ tool if, for any reason, the reverberation time measurements cannot be carried out using the standardized methods. The methods compared in this paper include the standardized methods (EN ISO 3382:2000), namely the impulse response measured with pink noise, exponential sweep, MLS, but also pistol shots of different calibers, balloon bursts, gated external pink noise, and the B\&K filtered burst method. In order to make the comparison, the measurements were performed in four acoustically very different spaces - a rather small and well-damped listening room, a much bigger damped listening room, a rather reverberant atrium, and a large and very reverberant shoebox-shaped room. The results were evaluated according to signal-to-noise ratio criterion as well. Special attention has been given to the influence of room modes on measurement results.},
  issue = {5\_Supplement},
  langid = {english}
}

@book{kaplanArtificialIntelligenceWhat2016,
  title = {Artificial {{Intelligence}}: {{What Everyone Needs}} to {{Know}}®{{What Everyone Needs}} to {{Know}}®},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Kaplan, Jerry},
  date = {2016-11-24},
  doi = {10.1093/wentk/9780190602383.001.0001},
  abstract = {Over the coming decades, Artificial Intelligence will profoundly impact the way we live, work, wage war, play, seek a mate, educate our young, and care for our elderly. It is likely to greatly increase our aggregate wealth, but it will also upend our labor markets, reshuffle our social order, and strain our private and public institutions. Eventually it may alter how we see our place in the universe, as machines pursue goals independent of their creators and outperform us in domains previously believed to be the sole dominion of humans. Whether we regard them as conscious or unwitting, revere them as a new form of life or dismiss them as mere clever appliances, is beside the point. They are likely to play an increasingly critical and intimate role in many aspects of our lives. The emergence of systems capable of independent reasoning and action raises serious questions about just whose interests they are permitted to serve, and what limits our society should place on their creation and use. Deep ethical questions that have bedeviled philosophers for ages will suddenly arrive on the steps of our courthouses. Can a machine be held accountable for its actions? Should intelligent systems enjoy independent rights and responsibilities, or are they simple property? Who should be held responsible when a self-driving car kills a pedestrian? Can your personal robot hold your place in line, or be compelled to testify against you? If it turns out to be possible to upload your mind into a machine, is that still you? The answers may surprise you.},
  isbn = {978-0-19-060238-3}
}

@book{kapoorDeepLearningTensorFlow2019,
  title = {Deep {{Learning}} with {{TensorFlow}} 2 and {{Keras}}: {{Regression}}, {{ConvNets}}, {{GANs}}, {{RNNs}}, {{NLP}}, and More with {{TensorFlow}} 2 and the {{Keras API}}, 2nd {{Edition}}},
  shorttitle = {Deep {{Learning}} with {{TensorFlow}} 2 and {{Keras}}},
  author = {Kapoor, Amita and Guili, Antonio and Pal, Sujit},
  date = {2019-12-29},
  abstract = {Deep Learning with TensorFlow 2 and Keras, Second Edition teaches neural networks and deep learning techniques alongside TensorFlow (TF) and Keras. You'll learn how to write deep learning applications in the most powerful, popular, and scalable machine learning stack available. TensorFlow is the machine learning library of choice for professional applications, while Keras offers a simple and powerful Python API for accessing TensorFlow. TensorFlow 2 provides full Keras integration, making advanced machine learning easier and more convenient than ever before. This book also introduces neural networks with TensorFlow, runs through the main applications (regression, ConvNets (CNNs), GANs, RNNs, NLP), covers two working example apps, and then dives into TF in production, TF mobile, and using TensorFlow with AutoML.},
  isbn = {978-1-83882-341-2}
}

@article{kimAcousticRoomModelling2021,
  title = {Acoustic {{Room Modelling Using}} 360 {{Stereo Cameras}}},
  author = {Kim, Hansung and Remaggi, Luca and Fowler, Sam and Jackson, Philip Jb and Hilton, Adrian},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  shortjournal = {IEEE Trans. Multimedia},
  volume = {23},
  pages = {4117--4130},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/tmm.2020.3037537},
  url = {https://ieeexplore.ieee.org/document/9257197/},
  urldate = {2025-07-26},
  abstract = {In this paper we propose a pipeline for estimating acoustic 3D room structure with geometry and attribute prediction using spherical 360◦ cameras. Instead of setting microphone arrays with loudspeakers to measure acoustic parameters for specific rooms, a simple and practical single-shot capture of the scene using a stereo pair of 360 cameras can be used to simulate those acoustic parameters. We assume that the room and objects can be represented as cuboids aligned to the main axes of the room coordinate (Manhattan world). The scene is captured as a stereo pair using off-the-shelf consumer spherical 360 cameras. A cuboid-based 3D room geometry model is estimated by correspondence matching between captured images and semantic labelling using a convolutional neural network (SegNet). The estimated geometry is used to produce frequency-dependent acoustic predictions of the scene. This is, to our knowledge, the first attempt in the literature to use visual geometry estimation and object classification algorithms to predict acoustic properties. Results are compared to measurements through calculated reverberant spatial audio object parameters used for reverberation reproduction customized to the given loudspeaker set up.},
  langid = {english}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-30},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2025-07-31},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning}
}

@article{krokstadCalculatingAcousticalRoom1968,
  title = {Calculating the Acoustical Room Response by the Use of a Ray Tracing Technique},
  author = {Krokstad, A. and Strom, S. and Sørsdal, S.},
  date = {1968-07-01},
  journaltitle = {Journal of Sound and Vibration},
  shortjournal = {Journal of Sound and Vibration},
  volume = {8},
  number = {1},
  pages = {118--125},
  issn = {0022-460X},
  doi = {10.1016/0022-460X(68)90198-3},
  url = {https://www.sciencedirect.com/science/article/pii/0022460X68901983},
  urldate = {2025-07-27},
  abstract = {The distribution of early reflected sound over the audience areas in concert halls is investigated, especially with respect to the shape of halls. The study is based on geometrical acoustics, using a ray tracing technique. The sound intensity is calculated by digital computer, and a graphical representation obtained. Test results for a rectangular and a fanshaped hall are given.}
}

@book{kuttruffRoomAcoustics2006,
  title = {Room Acoustics},
  author = {Kuttruff, Heinrich},
  date = {2006},
  edition = {4. ed., transferred to digital printing},
  publisher = {Taylor \& Francis},
  location = {London New York},
  isbn = {978-0-419-24580-3},
  langid = {english},
  pagetotal = {349}
}

@article{lollmannImprovedAlgorithmBlind2010,
  title = {An Improved Algorithm for Blind Reverberation Time Estimation},
  author = {Löllmann, Heinrich and Yılmaz, Emre and Jeub, Marco and Vary, Peter},
  date = {2010-08-30},
  abstract = {An improved algorithm for the estimation of the reverberation time (RT) from reverberant speech signals is presented. This blind estimation of the RT is based on a simple statistical model for the sound decay such that the RT can be estimated by means of a maximum-likelihood (ML) estimator. The proposed algorithm has a significantly lower computational complexity than previous ML-based algorithms for RT estima-tion. This is achieved by a downsampling operation and a simple pre-selection of possible sound decays. The new algorithm is more suitable to track time-varying RTs than related approaches. In addition, it can also estimate the RT in the presence of (moderate) background noise. The proposed algorithm can be employed to measure the RT of rooms from sound recordings without using a dedicated measurement setup. Another possible application is its use within speech dereverberation systems for hands-free devices or digital hearing aids.}
}

@book{longArchitecturalAcoustics2006,
  title = {Architectural {{Acoustics}}},
  author = {Long, Marshall},
  date = {2006-01-01},
  url = {http://archive.org/details/ArchitecturalAcoustics_201901},
  urldate = {2025-07-27},
  abstract = {From the Applications of Modern Acoustics Series Edited by Moises Levy and Richard Stern},
  langid = {english},
  keywords = {architecture}
}

@online{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2025-07-31},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control}
}

@article{lubeckBinauralReproductionMicrophone2025,
  title = {Binaural {{Reproduction}} of {{Microphone Array Recordings With 2D Video}} in {{Mixed Reality}}},
  author = {Lübeck, Tim and Ben-Hur, Zamir and Lou Alon, David and Crukley, Jeffery},
  date = {2025-07-07},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {J. Audio Eng. Soc.},
  volume = {73},
  number = {7/8},
  pages = {461--470},
  publisher = {Audio Engineering Society},
  issn = {1549-4950},
  doi = {10.17743/jaes.2022.0213},
  url = {https://aes2.org/publications/elibrary-page/?id=22924},
  urldate = {2025-07-18},
  abstract = {Head-worn devices equipped with microphone arrays and cameras can be used to capture the experience from a user’s perspective and reproduce it in virtual, mixed, or augmented reality. A concept that has recently introduced is to present the video capture as a 2D video screen augmented into the real-world environment through a mixed reality headset. This study presents such a system for reproducing audio and video capture from glasses arrays as a video “augment” along with binaural audio. Results of an initial listening experiment are presented, evaluating different state-of-the-art methods for binaural rendering. A stereo rendering through virtual loudspeakers attached to the video “augment” is compared with head-locked and world-locked binaural syntheses based on a binaural beamforming approach. The results suggest that listeners rated beamforming-based reproduction higher than stereo rendering. World-locked rendering was not rated significantly better than the head-locked version.},
  langid = {english}
}

@article{lundebyUncertaintiesMeasurementsRoom1995,
  title = {Uncertainties of {{Measurements}} in {{Room Acoustics}}},
  author = {Lundeby, Anders and Vorländer, Michael and Vigran, Tor Erik and Bietz, Heinrich},
  date = {1995},
  journaltitle = {Acustica : international journal on acoustics},
  volume = {81},
  number = {4},
  pages = {344--355}
}

@book{marburgComputationalAcousticsNoise2008,
  title = {Computational {{Acoustics}} of {{Noise Propagation}} in {{Fluids}} - {{Finite}} and {{Boundary Element Methods}}},
  author = {Marburg, Steffen and Nolte, Bodo},
  date = {2008-03-01},
  doi = {10.1007/978-3-540-77448-8_18},
  abstract = {Half–space acoustical problems are of great importance, since sound fields caused by radiation or scattering from complex structures are seldom found in the boundless infinite three–dimensional space. In many cases, a flat ground with a certain surface impedance confines the acoustical space as it encounters for example when treating problems of outdoor sound propagation. Whereas the boundary element method (BEM) is a mighty tool for predicting sound fields in unlimited space, the occurrence of an infinite plane and the associated additional discretization effort weakens the advantages of the classical BEM. One possible remedy is to use a Green’s function as a building block for the BEM which automatically satisfies the boundary condition at the infinite plane and to incorporate it into the BEM. In the present article, such a half–space Green’s function will be constructed, inserted into a direct BEM formulation, and positively tested by solving numerically several different problems of acoustical sound propagation over impedance planes.},
  isbn = {978-3-540-77447-1}
}

@inreference{MeanAbsoluteError2025,
  title = {Mean Absolute Error},
  booktitle = {Wikipedia},
  date = {2025-02-16T18:40:23Z},
  url = {https://en.wikipedia.org/w/index.php?title=Mean_absolute_error&oldid=1276071917},
  urldate = {2025-07-22},
  abstract = {In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. MAE is calculated as the sum of absolute errors (i.e., the Manhattan distance) divided by the sample size:                                   M           A           E                  =                                                                 ∑                                    i                   =                   1                                                     n                                                                |                                                         y                                            i                                                           −                                        x                                            i                                                                          |                                         n                             =                                                                 ∑                                    i                   =                   1                                                     n                                                                |                                    e                                        i                                                     |                                         n                             .                 \{\textbackslash displaystyle \textbackslash mathrm \{MAE\} =\{\textbackslash frac \{\textbackslash sum \_\{i=1\}\textasciicircum\{n\}\textbackslash left|y\_\{i\}-x\_\{i\}\textbackslash right|\}\{n\}\}=\{\textbackslash frac \{\textbackslash sum \_\{i=1\}\textasciicircum\{n\}\textbackslash left|e\_\{i\}\textbackslash right|\}\{n\}\}.\}    It is thus an arithmetic average of the absolute errors                                    |                             e                        i                                        |                  =                    |                             y                        i                             −                    x                        i                                        |                          \{\textbackslash displaystyle |e\_\{i\}|=|y\_\{i\}-x\_\{i\}|\}    , where                                    y                        i                                     \{\textbackslash displaystyle y\_\{i\}\}     is the prediction and                                    x                        i                                     \{\textbackslash displaystyle x\_\{i\}\}     the true value. Alternative formulations may include relative frequencies as weight factors. The mean absolute error uses the same scale as the data being measured. This is known as a scale-dependent accuracy measure and therefore cannot be used to make comparisons between predicted values that use different scales. The mean absolute error is a common measure of forecast error in time series analysis, sometimes used in confusion with the more standard definition of mean absolute deviation. The same confusion exists more generally.},
  langid = {english},
  annotation = {Page Version ID: 1276071917}
}

@inreference{MeanSquaredError2025,
  title = {Mean Squared Error},
  booktitle = {Wikipedia},
  date = {2025-05-11T12:45:21Z},
  url = {https://en.wikipedia.org/w/index.php?title=Mean_squared_error&oldid=1289884489},
  urldate = {2025-07-22},
  abstract = {In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the true value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. In machine learning, specifically empirical risk minimization, MSE may refer to the empirical risk (the average loss on an observed data set), as an estimate of the true MSE (the true risk: the average loss on the actual population distribution). The MSE is a measure of the quality of an estimator.  As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero. The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the true value). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error.},
  langid = {english},
  annotation = {Page Version ID: 1289884489}
}

@inproceedings{milneUseArtificialIntelligence2020,
  title = {Use of {{Artificial Intelligence}} in {{Room Acoustics Prediction Using}} a {{Photograph}}},
  booktitle = {Reproduced {{Sound}} 2020},
  author = {Milne, D and Davison, L and Ausiello, L},
  date = {2020-12-04},
  publisher = {Institute of Acoustics},
  location = {Virtual},
  doi = {10.25144/13372},
  url = {https://www.ioa.org.uk/catalogue/paper/use-artificial-intelligence-room-acoustics-prediction-using-photograph},
  urldate = {2025-07-26},
  eventtitle = {Reproduced {{Sound}} 2020},
  langid = {english}
}

@inproceedings{nanniCombiningVisualAcoustic2016,
  title = {Combining {{Visual}} and {{Acoustic Features}} for {{Bird Species Classification}}},
  author = {Nanni, Loris and Costa, Yandre and Lucio, Diego and Silla, Carlos and Brahnam, Sheryl},
  date = {2016-11-01},
  pages = {396--401},
  doi = {10.1109/ICTAI.2016.0067}
}

@online{owensVisuallyIndicatedSounds2016,
  title = {Visually {{Indicated Sounds}}},
  author = {Owens, Andrew and Isola, Phillip and McDermott, Josh and Torralba, Antonio and Adelson, Edward H. and Freeman, William T.},
  date = {2016-04-30},
  eprint = {1512.08512},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.08512},
  url = {http://arxiv.org/abs/1512.08512},
  urldate = {2025-07-26},
  abstract = {Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a "real or fake" psychophysical experiment, and that they convey significant information about material properties and physical interactions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound}
}

@article{pengQuantifyingJustNoticeable,
  title = {Quantifying the Just Noticeable Difference of Reverberation Time with Band-Limited Noise Centered around 1000 {{Hz}} Using a Transformed up-down Adaptive Method},
  author = {Peng, Z Ellen and Blevins, Matthew G and Buck, Adam T and Wang, Lily M},
  abstract = {This study seeks to quantify the just noticeable difference (JND) of reverberation time (RT) using band-limited noise. ISO 3382-1 lists the JND of reverberation metrics at 5\% based on work by Seraphim (1958). However, others have found the JND of RT to be higher from 6\% to 39\%. Many of these studies utilized band-limited stimuli, e.g. speech, music motifs and bandlimited noise. A previous study by the authors conducted on 30 subjects using white noise demonstrated a JND of RT at 22\%. To further verify these results and investigate potential upward frequency masking, the present study was conducted following the same methodology but using octave-band limited noise centered at 1000 Hz instead of white noise. Binaural room impulse responses (BRIR) were created from the Elmia concert hall in ODEON by uniformly varying absorption coefficients across all surfaces and frequencies to achieve the desired RTs. The desired RTs varied around three reference values (1, 2, and 3 seconds), with eight samples approaching the reference RT from below and another eight approaching from above, at 4\% intervals of the reference RT. Auralizations of the BRIRs and 500 ms band-limited noise were randomly presented in a computer-based testing program using a three-interval one-up twodown forced choice method, while interleaving six staircase sequences (3 reference RT X 2 downward vs. upward approaching direction). Subjects were individually tested in a sound attenuated booth using headphones with flat frequency response. Results are presented and compared against those previously obtained using white noise.},
  langid = {english}
}

@article{perezMachinelearningbasedEstimationReverberation,
  title = {Machine-Learning-Based Estimation of Reverberation Time Using Room Geometry for Room Effect Rendering},
  author = {Pérez, Ricardo FALCÓN and Götz, Georg and Pulkki, Ville},
  abstract = {This work presents a machine-learning-based method to estimate the reverberation time of a virtual room for auralization purposes. The models take as input geometric features of the room and output the estimated reverberation time values as function of frequency. The proposed model is trained and evaluated using a novel dataset composed of real-world acoustical measurements of a single room with 832 different configurations of furniture and absorptive materials, for multiple loudspeaker positions. The method achieves a prediction accuracy of approximately 90\% for most frequency bands. Furthermore, when comparing against the Sabine and Eyring methods, the proposed approach exhibits a much higher accuracy, especially at low frequencies.},
  langid = {english}
}

@article{pratesBlindEstimationReverberation,
  title = {Blind Estimation of Reverberation Time by Neural Networks},
  author = {Prates, Rodrigo L and Petraglia, Mariane R and Torres, Julio C B and Petraglia, Antonio},
  abstract = {In this paper we investigate a procedure for blind estimation of room reverberation time, that is, without a priori knowledge of the room impulse response. A neural network based method is proposed, which is robust to noise and to abrupt variations over time of the acquired signal. The input features of the neural network are calculated in the frequency domain, where training samples are generated in the Mel scale. A competent network architecture (i.e., with appropriate number and type of hidden layers and number of neurons comprising each layer), as well as the best training algorithm and regularization technique, are investigated. The use of cross-entropy error during training gives better results than those obtained by mean squared error. Comparative results are presented, considering two previously proposed blind methods: a deep neural network technique and a spectral decay distribution method.},
  langid = {english}
}

@book{pregoBlindEstimatorsReverberation2015,
  title = {Blind Estimators for Reverberation Time and Direct-to-Reverberant Energy Ratio Using Subband Speech Decomposition},
  author = {Prego, Thiago and Lima, Amaro and Zambrano-Lopez, Rafael and Netto, Sergio},
  date = {2015-10-01},
  pages = {5},
  doi = {10.1109/WASPAA.2015.7336954},
  pagetotal = {1}
}

@article{prodiSlightIncreaseReverberation2022,
  title = {A {{Slight Increase}} in {{Reverberation Time}} in the {{Classroom Affects Performance}} and {{Behavioral Listening Effort}}},
  author = {Prodi, Nicola and Visentin, Chiara},
  date = {2022},
  journaltitle = {Ear and Hearing},
  shortjournal = {Ear Hear},
  volume = {43},
  number = {2},
  eprint = {34369418},
  eprinttype = {pubmed},
  pages = {460--476},
  issn = {1538-4667},
  doi = {10.1097/AUD.0000000000001110},
  abstract = {OBJECTIVES: The purpose of this study was to investigate the effect of a small change in reverberation time (from 0.57 to 0.69\,s) in a classroom on children's performance and listening effort. Aiming for ecological listening conditions, the change in reverberation time was combined with the presence or absence of classroom noise. In three academic tasks, the study examined whether the effect of reverberation was modulated by the presence of noise and depended on the children's age. DESIGN: A total of 302 children (aged 11-13 years, grades 6-8) with normal hearing participated in the study. Three typical tasks of daily classroom activities (speech perception, sentence comprehension, and mental calculation) were administered to groups of children in two listening conditions (quiet and classroom noise). The experiment was conducted inside real classrooms, where reverberation time was controlled. The outcomes considered were task accuracy and response times (RTs), the latter taken as a behavioral proxy for listening effort. Participants were also assessed on reading comprehension and math fluency. To investigate the impact of noise and/or reverberation, these two scores were entered in the statistical model to control for individual child's general academic abilities. RESULTS: While the longer reverberation time did not significantly affect accuracy or RTs under the quiet condition, it had several effects when in combination with classroom noise, depending on the task measured. A significant drop in accuracy with a longer reverberation time emerged for the speech perception task, but only for the grade 6 children. The effect on accuracy of a longer reverberation time was nonsignificant for sentence comprehension (always at ceiling), and depended on the children's age in the mental calculation task. RTs were longer for moderate than for short reverberation times in the speech perception and sentence comprehension tasks, while there was no significant effect of the different reverberation times on RTs in the mental calculation task. CONCLUSIONS: The results indicate small, but statistically significant, effects of a small change in reverberation time on listening effort as well as accuracy for children aged 11 to 13 performing typical tasks of daily classroom activities. Thus, the results extend previous findings in adults to children as well. The findings also contribute to a better understanding of the practical implications and importance of optimal ranges of reverberation time in classrooms. A comparison with previous studies underscored the importance of early reflections as well as reverberation times in classrooms.},
  langid = {english},
  keywords = {Adult,Auditory Perception,Child,Humans,Language,Listening Effort,Noise,Speech Perception}
}

@software{PyfarPyfar2025,
  title = {Pyfar/Pyfar},
  date = {2025-07-04T13:02:59Z},
  origdate = {2020-10-27T19:13:03Z},
  url = {https://github.com/pyfar/pyfar},
  urldate = {2025-07-19},
  abstract = {python package for acoustics research},
  organization = {pyfar},
  keywords = {audio-in-out,digital-signal-processing,plotting}
}

@software{PyfarPyrato2025,
  title = {Pyfar/Pyrato},
  date = {2025-06-30T13:11:13Z},
  origdate = {2020-09-10T14:58:47Z},
  url = {https://github.com/pyfar/pyrato},
  urldate = {2025-07-19},
  abstract = {Python Room Acoustics Tools},
  organization = {pyfar}
}

@online{PyratoEnergyDecay,
  title = {Pyrato — {{Energy Decay Curve Chu Lundeby}}},
  url = {https://pyrato.readthedocs.io/en/latest/modules/pyrato.html#pyrato.energy_decay_curve_chu_lundeby},
  urldate = {2025-07-31}
}

@article{ratnamBlindEstimationReverberation2003,
  title = {Blind Estimation of Reverberation Time},
  author = {Ratnam, Rama and Jones, Douglas L. and Wheeler, Bruce C. and O’Brien, William D. and Lansing, Charissa R. and Feng, Albert S.},
  date = {2003-11-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {114},
  number = {5},
  pages = {2877--2892},
  publisher = {Acoustical Society of America (ASA)},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.1616578},
  url = {https://pubs.aip.org/jasa/article/114/5/2877/547709/Blind-estimation-of-reverberation-time},
  urldate = {2025-07-20},
  abstract = {The reverberation time (RT) is an important parameter for characterizing the quality of an auditory space. Sounds in reverberant environments are subject to coloration. This affects speech intelligibility and sound localization. Many state-of-the-art audio signal processing algorithms, for example in hearing-aids and telephony, are expected to have the ability to characterize the listening environment, and turn on an appropriate processing strategy accordingly. Thus, a method for characterization of room RT based on passively received microphone signals represents an important enabling technology. Current RT estimators, such as Schroeder’s method, depend on a controlled sound source, and thus cannot produce an online, blind RT estimate. Here, a method for estimating RT without prior knowledge of sound sources or room geometry is presented. The diffusive tail of reverberation was modeled as an exponentially damped Gaussian white noise process. The time-constant of the decay, which provided a measure of the RT, was estimated using a maximum-likelihood procedure. The estimates were obtained continuously, and an order-statistics filter was used to extract the most likely RT from the accumulated estimates. The procedure was illustrated for connected speech. Results obtained for simulated and real room data are in good agreement with the real RT values.},
  langid = {english}
}

@online{ratnarajahAVRIRAudioVisualRoom2024,
  title = {{{AV-RIR}}: {{Audio-Visual Room Impulse Response Estimation}}},
  shorttitle = {{{AV-RIR}}},
  author = {Ratnarajah, Anton and Ghosh, Sreyan and Kumar, Sonal and Chiniya, Purva and Manocha, Dinesh},
  date = {2024-04-23},
  eprint = {2312.00834},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.00834},
  url = {http://arxiv.org/abs/2312.00834},
  urldate = {2025-07-26},
  abstract = {Accurate estimation of Room Impulse Response (RIR), which captures an environment's acoustic properties, is important for speech processing and AR/VR applications. We propose AV-RIR, a novel multi-modal multi-task learning approach to accurately estimate the RIR from a given reverberant speech signal and the visual cues of its corresponding environment. AV-RIR builds on a novel neural codec-based architecture that effectively captures environment geometry and materials properties and solves speech dereverberation as an auxiliary task by using multi-task learning. We also propose Geo-Mat features that augment material information into visual cues and CRIP that improves late reverberation components in the estimated RIR via image-to-RIR retrieval by 86\%. Empirical results show that AV-RIR quantitatively outperforms previous audio-only and visual-only approaches by achieving 36\% - 63\% improvement across various acoustic metrics in RIR estimation. Additionally, it also achieves higher preference scores in human evaluation. As an auxiliary benefit, dereverbed speech from AV-RIR shows competitive performance with the state-of-the-art in various spoken language processing tasks and outperforms reverberation time error score in the real-world AVSpeech dataset. Qualitative examples of both synthesized reverberant speech and enhanced speech can be found at https://www.youtube.com/watch?v=tTsKhviukAE.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Sound}
}

@online{ReduceLROnPlateauPyTorch27,
  title = {{{ReduceLROnPlateau}} — {{PyTorch}} 2.7 Documentation},
  url = {https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html},
  urldate = {2025-07-31}
}

@online{ReverberationTimeRoom,
  title = {Reverberation {{Time}} in {{Room Acoustics}}},
  url = {https://www.larsondavis.com/learn/building-acoustics/Reverberation-Time-in-Room-Acoustics},
  urldate = {2025-07-29},
  abstract = {Need to measure reverberation time with your sound level meter? Common room acoustics measurements of RT60, T30, T20 explained.},
  langid = {english},
  organization = {Larson Davis}
}

@online{richterEARSAnechoicFullband2024,
  title = {{{EARS}}: {{An Anechoic Fullband Speech Dataset Benchmarked}} for {{Speech Enhancement}} and {{Dereverberation}}},
  shorttitle = {{{EARS}}},
  author = {Richter, Julius and Wu, Yi-Chiao and Krenn, Steven and Welker, Simon and Lay, Bunlong and Watanabe, Shinji and Richard, Alexander and Gerkmann, Timo},
  date = {2024-06-11},
  eprint = {2406.06185},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2406.06185},
  url = {http://arxiv.org/abs/2406.06185},
  urldate = {2025-07-19},
  abstract = {We release the EARS (Expressive Anechoic Recordings of Speech) dataset, a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totaling in 100 hours of clean, anechoic speech data. The dataset covers a large range of different speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech. We benchmark various methods for speech enhancement and dereverberation on the dataset and evaluate their performance through a set of instrumental metrics. In addition, we conduct a listening test with 20 participants for the speech enhancement task, where a generative method is preferred. We introduce a blind test set that allows for automatic online evaluation of uploaded data. Dataset download links and automatic evaluation server can be found online1.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@online{RoomImpulseResponse,
  title = {Room Impulse Response Simulation with Stochastic Ray Tracing - MATLAB \&amp; Simulink},
  url = {https://www.mathworks.com/help/audio/ug/room-impulse-response-simulation-with-stochastic-ray-tracing.html},
  urldate = {2025-07-16},
  abstract = {Use stochastic ray tracing to simulate the impulse response of a simple room.},
  langid = {ngerman}
}

@online{RoomImpulseResponsea,
  title = {Room Impulse Response Simulation with the Image-Source Method and HRTF Interpolation - MATLAB \&amp; Simulink},
  url = {https://www.mathworks.com/help/audio/ug/room-impulse-response-simulation-with-image-source-method-and-hrtf-interpolation.html},
  urldate = {2025-07-16},
  abstract = {Simulate the impulse response of a "shoebox" (cuboid) empty room.},
  langid = {ngerman}
}

@online{RT60ReverberationTime,
  title = {{{RT60 Reverberation Time}}},
  url = {https://svantek.com/academy/rt60-reverberation-time/},
  urldate = {2025-07-29},
  abstract = {RT60 Reverberation Time is the room's acoustic parameter. It is the time for the sound energy to decrease by 60 dB after the source stops.},
  langid = {american},
  organization = {SVANTEK - Sound and Vibration}
}

@book{sabineCollectedPapersAcoustics1922,
  title = {Collected Papers on Acoustics},
  author = {Sabine, Wallace Clement},
  namea = {{University of California Libraries}},
  nameatype = {collaborator},
  date = {1922},
  publisher = {Cambridge : Harvard University Press},
  url = {http://archive.org/details/collectedpaperso00sabi},
  urldate = {2025-07-16},
  abstract = {ix, 279 p. 27 cm},
  langid = {english},
  pagetotal = {308},
  keywords = {Architectural acoustics}
}

@inbook{sabineReverberation1922,
  title = {Reverberation},
  booktitle = {Collected Papers on Acoustics},
  namea = {{University of California Libraries}},
  nameatype = {collaborator},
  date = {1922},
  pages = {3--68},
  publisher = {Cambridge : Harvard University Press},
  url = {http://archive.org/details/collectedpaperso00sabi},
  urldate = {2025-07-16},
  bookauthor = {Sabine, Wallace Clement},
  langid = {english},
  keywords = {Architectural acoustics}
}

@article{saviojaOverviewGeometricalRoom2015,
  title = {Overview of Geometrical Room Acoustic Modeling Techniques},
  author = {Savioja, Lauri and Svensson, U. Peter},
  date = {2015-08-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {138},
  number = {2},
  pages = {708--730},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.4926438},
  url = {https://pubs.aip.org/jasa/article/138/2/708/917382/Overview-of-geometrical-room-acoustic-modeling},
  urldate = {2025-07-27},
  abstract = {Computerized room acoustics modeling has been practiced for almost 50 years up to date. These modeling techniques play an important role in room acoustic design nowadays, often including auralization, but can also help in the construction of virtual environments for such applications as computer games, cognitive research, and training. This overview describes the main principles, landmarks in the development, and state-of-the-art for techniques that are based on geometrical acoustics principles. A focus is given to their capabilities to model the different aspects of sound propagation: specular vs diffuse reflections, and diffraction.},
  langid = {english}
}

@article{schaabDemonstratorAuralizationControl2017,
  title = {Demonstrator for the Auralization and Control of the Room Divergence Effect},
  author = {Schaab, M and Dobmeier, V and Werner, S and Klein, F},
  date = {2017},
  abstract = {The goal of binaural headphone reproduction is to synthesize a virtual room or to resynthesize the acoustics of a real room. Former research has shown, that the acoustical divergence between the room presented over headphones and the actual listening room can violate the expectations of the listener. In this case, the perceived quality of the synthesized room is degraded despite of a technical correct synthesis of the ear signals. This effect is called room divergence effect and is measured in a reduction of externalization of sound events. This publication describes a demonstrator which auralizes this effect. For this purpose a 5 channel loudspeaker setup is measured with a KEMAR artificial head in two rooms. Additionally three algorithms are implemented to calculate virtual rooms in between the measured rooms. By listening to the unmodified rooms measurements and their modifications differences in externalization are distinguishable. The influence of each algorithm on externalization in a divergent listening scenario is evaluated in a listening test with 14 participants.},
  langid = {english}
}

@inproceedings{scheiblerPyroomacousticsPythonPackage2018,
  title = {Pyroomacoustics: {{A Python Package}} for {{Audio Room Simulation}} and {{Array Processing Algorithms}}},
  shorttitle = {Pyroomacoustics},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Scheibler, Robin and Bezzam, Eric and Dokmanić, Ivan},
  date = {2018-04},
  pages = {351--355},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8461310},
  url = {https://ieeexplore.ieee.org/document/8461310},
  urldate = {2025-07-27},
  abstract = {We present pyroomacoustics, a software package aimed at the rapid development and testing of audio array processing algorithms. The content of the package can be divided into three main components: an intuitive Python object-oriented interface to quickly construct different simulation scenarios involving multiple sound sources and microphones in 2D and 3D rooms; a fast C implementation of the image source model for general polyhedral rooms to efficiently generate room impulse responses and simulate the propagation between sources and receivers; and finally, reference implementations of popular algorithms for beamforming, direction finding, and adaptive filtering. Together, they form a package with the potential to speed up the time to market of new algorithms by significantly reducing the implementation overhead in the performance evaluation step.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {Array signal processing,Generators,Mathematical model,Microphones,Object oriented modeling,Python,rapid prototyping,reference implementations,reproducibility,RIR,simulation,Three-dimensional displays}
}

@article{selvarajuGradCAMVisualExplanations2016,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2016},
  doi = {10.48550/ARXIV.1610.02391},
  url = {https://arxiv.org/abs/1610.02391},
  urldate = {2025-05-23},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.}
}

@article{shelleyOpenAIR129thAudio2010,
  title = {{{OpenAIR}}: 129th {{Audio Engineering Society Convention}} 2010},
  shorttitle = {{{OpenAIR}}},
  author = {Shelley, Simon and Murphy, Damian T.},
  date = {2010},
  journaltitle = {129th Audio Engineering Society Convention 2010},
  series = {129th {{Audio Engineering Society Convention}} 2010},
  pages = {1270--1278},
  issn = {9781617821943},
  url = {http://www.scopus.com/inward/record.url?scp=84866013022&partnerID=8YFLogxK},
  urldate = {2025-07-19},
  abstract = {There have been many recent initiatives to capture the impulse responses of important or interesting acoustic spaces, although not all of this data has been made more widely available to researchers interested in auralization. This paper presents the Open Acoustic Impulse Response (OpenAIR) Library, a new online resource allowing users to share impulse responses and related acoustical information. Open-source software is provided, enabling the user to render the acoustical data using various auralization strategies. Software tools and guidelines for the process of impulse response capture are also provided, aiming to disseminate best practice. The database can accommodate impulse response datasets captured according to different measurement techniques and the use of robust spatial audio coding formats is also considered for the distribution of this type of information. Visitors to the resource can search for acoustical data using keywords, and can also browse uploaded datasets on a world map.}
}

@article{shinn-cunninghamBarbaraShinnCunningham2015,
  title = {Barbara {{Shinn-Cunningham}}},
  author = {Shinn-Cunningham, Barbara},
  date = {2015-06},
  journaltitle = {Current Biology},
  volume = {25},
  number = {11},
  pages = {R442-R444},
  publisher = {Elsevier BV},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2015.02.060},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982215002298},
  urldate = {2025-07-18},
  abstract = {One aspect of hearing that has received relatively little attention by traditional psychophysicists is how echoes and reverberation in everyday spaces affect perception. In the ordinary world, echoes and reverberation are ubiquitous and influence the signals reaching the listener, the processing of these signals by the brain, and the resulting perception of both sound sources and the environment. Many aspects of the signals reaching the ear are altered or "distorted" by echoes and reverberation, including spectral content, interaural differences, and temporal structure. As a result, echoes and reverberation could influence many aspects of perception, including spatial hearing in direction and distance, speech intelligibility, and spatial unmasking. This paper reviews results from a number of studies examining how the acoustics of ordinary rooms affect various aspects of the signals reaching a listener's ears as well as resulting perception. While the acoustic effects of reverberant energy are often pronounced, performance on most behavioral tasks is relatively robust to these effects. These perceptual results suggest that listeners may not simply be adept at ignoring the signal distortion caused by ordinary room acoustics, but may be adapted to deal with its presence. These results are important for designing truly immersive spatial auditory displays, because they demonstrate the importance of reverberant energy for achieving a realistic, immersive experience.},
  langid = {english}
}

@online{SimultaneousMeasurementImpulse,
  title = {Simultaneous {{Measurement}} of {{Impulse Response}} and {{Distortion With}} a {{Swept-Sine Technique}}},
  url = {https://www.researchgate.net/publication/2456363_Simultaneous_Measurement_of_Impulse_Response_and_Distortion_With_a_Swept-Sine_Technique},
  urldate = {2025-07-29},
  abstract = {PDF | A novel measurement technique of the transfer function of weakly not-linear, approximately time-invariant systems is presented. The method is... | Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  organization = {ResearchGate}
}

@online{singhImage2ReverbCrossModalReverb2021,
  title = {{{Image2Reverb}}: {{Cross-Modal Reverb Impulse Response Synthesis}}},
  shorttitle = {{{Image2Reverb}}},
  author = {Singh, Nikhil and Mentch, Jeff and Ng, Jerry and Beveridge, Matthew and Drori, Iddo},
  date = {2021-08-13},
  eprint = {2103.14201},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.14201},
  url = {http://arxiv.org/abs/2103.14201},
  urldate = {2025-07-19},
  abstract = {Measuring the acoustic characteristics of a space is often done by capturing its impulse response (IR), a representation of how a full-range stimulus sound excites it. This work generates an IR from a single image, which can then be applied to other signals using convolution, simulating the reverberant characteristics of the space shown in the image. Recording these IRs is both time-intensive and expensive, and often infeasible for inaccessible locations. We use an end-to-end neural network architecture to generate plausible audio impulse responses from single images of acoustic environments. We evaluate our method both by comparisons to ground truth data and by human expert evaluation. We demonstrate our approach by generating plausible impulse responses from diverse settings and formats including well known places, musical halls, rooms in paintings, images from animations and computer games, synthetic environments generated from text, panoramic images, and video conference backgrounds.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{thompsonReviewFiniteelementMethods2006,
  title = {A Review of Finite-Element Methods for Time-Harmonic Acoustics. {{Journal}} of the {{Acoustical Society}} of {{America}}},
  author = {Thompson, Lonny},
  date = {2006-03-01},
  journaltitle = {Rz ADP Pages},
  shortjournal = {Rz ADP Pages},
  volume = {20},
  pages = {1315--1330},
  doi = {10.1121/1.2164987},
  abstract = {State-of-the-art finite-element methods for time-harmonic acoustics governed by the Helmholtz equation are reviewed. Four major current challenges in the field are specifically addressed: the effective treatment of acoustic scattering in unbounded domains, including local and nonlocal absorbing boundary conditions, infinite elements, and absorbing layers; numerical dispersion errors that arise in the approximation of short unresolved waves, polluting resolved scales, and requiring a large computational effort; efficient algebraic equation solving methods for the resulting complex-symmetric non-Hermitian matrix systems including sparse iterative and domain decomposition methods; and a posteriori error estimates for the Helmholtz operator required for adaptive methods. Mesh resolution to control phase error and bound dispersion or pollution errors measured in global norms for large wave numbers in finite-element methods are described. Stabilized, multiscale, and other wave-based discretization methods developed to reduce this error are reviewed. A review of finite-element methods for acoustic inverse problems and shape optimization is also given. © 2006 Acoustical Society of America.}
}

@online{Treble,
  title = {Treble},
  url = {https://www.treble.tech/},
  urldate = {2025-07-19}
}

@inproceedings{veauxVoiceBankCorpus2013,
  title = {The Voice Bank Corpus: {{Design}}, Collection and Data Analysis of a Large Regional Accent Speech Database},
  shorttitle = {The Voice Bank Corpus},
  booktitle = {2013 {{International Conference Oriental COCOSDA}} Held Jointly with 2013 {{Conference}} on {{Asian Spoken Language Research}} and {{Evaluation}} ({{O-COCOSDA}}/{{CASLRE}})},
  author = {Veaux, Christophe and Yamagishi, Junichi and King, Simon},
  date = {2013-11},
  pages = {1--4},
  doi = {10.1109/ICSDA.2013.6709856},
  url = {https://ieeexplore.ieee.org/document/6709856},
  urldate = {2025-07-19},
  abstract = {The University of Edinburgh has started the development of a new speech database, the Voice Bank corpus, specifically designed for the creation of personalised synthetic voices for individuals with speech disorders. This corpus already constitutes the largest corpora of British English currently in existence, with more than 300 hours of recordings from approximately 500 healthy speakers. New recordings are continuously being made in order to get the best coverage of the different combinations of regional accents, social classes, age and gender across Britain. This paper describes the motivation and the processes involved in the design and recording of this corpus as well as some analysis of its content. The paper concludes with our future plans to further extend this corpus and to overcome its current limitations.},
  eventtitle = {2013 {{International Conference Oriental COCOSDA}} Held Jointly with 2013 {{Conference}} on {{Asian Spoken Language Research}} and {{Evaluation}} ({{O-COCOSDA}}/{{CASLRE}})},
  keywords = {Corpus Design,Databases,Educational institutions,Hidden Markov models,Optimization,Recruitment,Speech,Speech synthesis,Speech Synthesis,Text Selection,Voice Banking}
}

@book{vorlanderAuralizationFundamentalsAcoustics2008,
  title = {Auralization: Fundamentals of Acoustics, Modelling, Simulation, Algorithms and Acoustic Virtual Reality},
  shorttitle = {Auralization},
  author = {Vorländer, Michael},
  date = {2008},
  edition = {1st ed},
  publisher = {Springer},
  location = {Berlin},
  isbn = {978-3-540-48829-3},
  pagetotal = {335},
  keywords = {Psychoacoustics,Sound in virtual reality}
}

@inproceedings{witmayerAutomatingMetadataLogging2018,
  title = {Automating {{Metadata Logging}} through {{Artificial Intelligence}}},
  author = {Witmayer, Christopher},
  date = {2018-10-01},
  pages = {1--10},
  doi = {10.5594/M001814}
}

@article{wittebolHybridRoomAcoustic2024,
  title = {A Hybrid Room Acoustic Modeling Approach Combining Image Source, Acoustic Diffusion Equation, and Time-Domain Discontinuous {{Galerkin}} Methods},
  author = {Wittebol, Wouter and Wang, Huiqing and Hornikx, Maarten and Calamia, Paul},
  date = {2024-07-05},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {223},
  pages = {110068},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2024.110068},
  url = {https://www.sciencedirect.com/science/article/pii/S0003682X24002196},
  urldate = {2025-07-16},
  abstract = {In this paper a hybrid model is introduced that constructs a broadband room impulse response using a geometrical (image source method) and a statistical method (acoustic diffusion equation) for the high-frequency range, supported by a wave-based method (time-domain discontinuous Galerkin method) for the low-frequency range. A crucial element concerns the construction of the high-frequency impulse response where a transition from a predominantly specular (image source) to a predominantly diffuse sound-field (diffusion equation) is required. To achieve this transition an analytical envelope is introduced. A key factor is the room-averaged scattering coefficient which accounts for all scattering behavior of the room and determines the speed of transition from a specular to a non-specular sound-field. To evaluate its performance, the model is compared to a broadband wave-based solver for two reference scenarios. The hybrid model shows promising results in terms of reverberation time (T20), center time (Ts) and bass-ratio (BR). Aspects such as the used geometrical complexity, the ‘room-averaged’ scattering coefficients, and other model simplifications and assumptions are discussed.},
  keywords = {Acoustic diffusion equation method,Hybrid,Image source method,Room acoustics,Time-domain discontinuous Galerkin method}
}

@article{wittebolHybridRoomAcoustic2024a,
  title = {A Hybrid Room Acoustic Modeling Approach Combining Image Source, Acoustic Diffusion Equation, and Time-Domain Discontinuous {{Galerkin}} Methods},
  author = {Wittebol, Wouter and Wang, Huiqing and Hornikx, Maarten and Calamia, Paul},
  date = {2024-07-05},
  journaltitle = {Applied Acoustics},
  volume = {223},
  number = {110068},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2024.110068},
  url = {http://www.scopus.com/inward/record.url?scp=85193450419&partnerID=8YFLogxK},
  urldate = {2025-07-27},
  abstract = {In this paper a hybrid model is introduced that constructs a broadband room impulse response using a geometrical (image source method) and a statistical method (acoustic diffusion equation) for the high-frequency range, supported by a wave-based method (time-domain discontinuous Galerkin method) for the low-frequency range. A crucial element concerns the construction of the high-frequency impulse response where a transition from a predominantly specular (image source) to a predominantly diffuse sound-field (diffusion equation) is required. To achieve this transition an analytical envelope is introduced. A key factor is the room-averaged scattering coefficient which accounts for all scattering behavior of the room and determines the speed of transition from a specular to a non-specular sound-field. To evaluate its performance, the model is compared to a broadband wave-based solver for two reference scenarios. The hybrid model shows promising results in terms of reverberation time (T20), center time (Ts) and bass-ratio (BR). Aspects such as the used geometrical complexity, the ‘room-averaged’ scattering coefficients, and other model simplifications and assumptions are discussed.},
  keywords = {Acoustic diffusion equation method,Hybrid,Image source method,Room acoustics,Time-domain discontinuous Galerkin method}
}

@online{yangDepthAnythingUnleashing2024,
  title = {Depth {{Anything}}: {{Unleashing}} the {{Power}} of {{Large-Scale Unlabeled Data}}},
  shorttitle = {Depth {{Anything}}},
  author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  date = {2024-04-07},
  eprint = {2401.10891},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.10891},
  url = {http://arxiv.org/abs/2401.10891},
  urldate = {2025-07-19},
  abstract = {This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (\textasciitilde 62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{zhouPlaces10Million2018,
  title = {Places: {{A}} 10 {{Million Image Database}} for {{Scene Recognition}}},
  shorttitle = {Places},
  author = {Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  date = {2018-06-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {40},
  number = {6},
  pages = {1452--1464},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2017.2723009},
  url = {https://ieeexplore.ieee.org/document/7968387/},
  urldate = {2025-07-31},
  abstract = {The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach nearhuman semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.},
  langid = {english}
}

@book{zotterAmbisonicsPractical3D2019,
  title = {Ambisonics: {{A Practical 3D Audio Theory}} for {{Recording}}, {{Studio Production}}, {{Sound Reinforcement}}, and {{Virtual Reality}}},
  shorttitle = {Ambisonics},
  author = {Zotter, Franz and Frank, Matthias},
  date = {2019},
  series = {Springer {{Topics}} in {{Signal Processing}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  issn = {1866-2609, 1866-2617},
  doi = {10.1007/978-3-030-17207-7},
  url = {http://link.springer.com/10.1007/978-3-030-17207-7},
  urldate = {2025-07-18},
  isbn = {978-3-030-17206-0 978-3-030-17207-7},
  langid = {english},
  keywords = {Ambisonics Book,Binaural Listening,Imaginary Loudspeakers,Open Access,Playback Technology,Psychoacoustics,Scene-based Sound,Spherical Array Processing,Surround Sound Recording}
}
