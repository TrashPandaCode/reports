@article{alhawitiAdvancesArtificialIntelligence2015,
  title = {Advances in {{Artificial Intelligence Using Speech Recognition}}},
  author = {Alhawiti, Khaled M.},
  date = {2015-06-03},
  journaltitle = {International Journal of Computer and Information Engineering},
  volume = {9},
  number = {6},
  pages = {1432--1435},
  url = {https://publications.waset.org/10001552/advances-in-artificial-intelligence-using-speech-recognition},
  urldate = {2025-07-27},
  abstract = {Advances in Artificial Intelligence Using Speech Recognition},
  langid = {english}
}

@article{ashqarImageBasedTomatoLeaves2019,
  title = {Image-{{Based Tomato Leaves Diseases Detection Using Deep Learning}}},
  author = {Ashqar, Belal and Abu-Naser, Samy},
  date = {2019-01-01},
  journaltitle = {International Journal of Engineering Research},
  shortjournal = {International Journal of Engineering Research},
  volume = {2},
  pages = {10--16},
  abstract = {Crop diseases are a key danger for food security, but their speedy identification still difficult in many portions of the world because of the lack of the essential infrastructure. The mixture of increasing worldwide smartphone dispersion and current advances in computer vision made conceivable by deep learning has cemented the way for smartphone-assisted disease identification. Using a public dataset of 9000 images of infected and healthy Tomato leaves collected under controlled conditions, we trained a deep convolutional neural network to identify 5 diseases. The trained model achieved an accuracy of 99.84\% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.}
}

@book{beranekConcertHallsOpera2010,
  title = {Concert {{Halls}} and Opera Houses: Music, Acoustics, and Architecture},
  shorttitle = {Concert {{Halls}} and Opera Houses},
  author = {Beranek, Leo L.},
  date = {2010},
  edition = {2., nd ed., softcover version of orig. hardcover ed. 2004},
  publisher = {Springer},
  location = {New York, NY},
  isbn = {978-1-4419-3038-5},
  langid = {english},
  pagetotal = {661}
}

@online{chenVisualAcousticMatching2022,
  title = {Visual {{Acoustic Matching}}},
  author = {Chen, Changan and Gao, Ruohan and Calamia, Paul and Grauman, Kristen},
  date = {2022-06-13},
  eprint = {2202.06875},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.06875},
  url = {http://arxiv.org/abs/2202.06875},
  urldate = {2025-07-26},
  abstract = {We introduce the visual acoustic matching task, in which an audio clip is transformed to sound like it was recorded in a target environment. Given an image of the target environment and a waveform for the source audio, the goal is to re-synthesize the audio to match the target room acoustics as suggested by its visible geometry and materials. To address this novel task, we propose a cross-modal transformer model that uses audio-visual attention to inject visual properties into the audio and generate realistic audio output. In addition, we devise a self-supervised training objective that can learn acoustic matching from in-the-wild Web videos, despite their lack of acoustically mismatched audio. We demonstrate that our approach successfully translates human speech to a variety of real-world environments depicted in images, outperforming both traditional acoustic matching and more heavily supervised baselines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inreference{CoefficientDetermination2025,
  title = {Coefficient of Determination},
  booktitle = {Wikipedia},
  date = {2025-07-22T00:31:45Z},
  url = {https://en.wikipedia.org/w/index.php?title=Coefficient_of_determination&oldid=1301836687},
  urldate = {2025-07-22},
  abstract = {In statistics, the coefficient of determination, denoted R2 or r2 and pronounced "R squared", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s). It is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. There are several definitions of R2 that are only sometimes equivalent. In simple linear regression (which includes an intercept), r2 is simply the square of the sample correlation coefficient (r), between the observed outcomes and the observed predictor values. If additional regressors are included, R2 is the square of the coefficient of multiple correlation. In both such cases, the coefficient of determination normally ranges from 0 to 1. There are cases where R2 can yield negative values. This can arise when the predictions that are being compared to the corresponding outcomes have not been derived from a model-fitting procedure using those data. Even if a model-fitting procedure has been used, R2 may still be negative, for example when linear regression is conducted without including an intercept, or when a non-linear function is used to fit the data. In cases where negative values arise, the mean of the data provides a better fit to the outcomes than do the fitted function values, according to this particular criterion. The coefficient of determination can be more intuitively informative than MAE, MAPE, MSE, and RMSE in regression analysis evaluation, as the former can be expressed as a percentage, whereas the latter measures have arbitrary ranges. It also proved more robust for poor fits compared to SMAPE on certain test datasets. When evaluating the goodness-of-fit of simulated (Ypred) versus measured (Yobs) values, it is not appropriate to base this on the R2 of the linear regression (i.e., Yobs= m·Ypred + b). The R2 quantifies the degree of any linear correlation between Yobs and Ypred, while for the goodness-of-fit evaluation only one specific linear correlation should be taken into consideration: Yobs = 1·Ypred + 0 (i.e., the 1:1 line).},
  langid = {english},
  annotation = {Page Version ID: 1301836687}
}

@book{cremerPrinciplesApplicationsRoom1982,
  title = {Principles and applications of room acoustics},
  author = {Cremer, Lothar and Müller, Helmut A.},
  date = {1982},
  publisher = {{Applied Science ; Sole distributor in the USA and Canada, Elsevier Science Pub. Co}},
  location = {London ; New York : New York, NY, USA},
  isbn = {978-0-85334-113-0 978-0-85334-114-7},
  langid = {eng ger},
  pagetotal = {2},
  keywords = {Architectural acoustics}
}

@article{eyringReverberationTimeDead1930,
  title = {Reverberation {{Time}} in “{{Dead}}” {{Rooms}}},
  author = {Eyring, Carl F.},
  date = {1930-01-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {1},
  pages = {217--241},
  issn = {0001-4966},
  doi = {10.1121/1.1915175},
  url = {https://doi.org/10.1121/1.1915175},
  urldate = {2025-07-16},
  issue = {2A}
}

@book{falconMachinelearningbasedEstimationReverberation2019,
  title = {Machine-Learning-Based Estimation of Reverberation Time Using Room Geometry for Room Effect Rendering},
  author = {Falcon, Ricardo and Götz, Georg and Pulkki, Ville},
  date = {2019-09-13},
  abstract = {This work presents a machine-learning-based method to estimate the reverberation time of a virtual room for auralization purposes. The models take as input geometric features of the room and output the estimated reverberation time values as function of frequency. The proposed model is trained and evaluated using a novel dataset composed of real-world acoustical measurements of a single room with 832 different configurations of furniture and absorptive materials, for multiple loudspeaker positions. The method achieves a prediction accuracy of approximately 90\% for most frequency bands. Furthermore, when comparing against the Sabine and Eyring methods, the proposed approach exhibits a much higher accuracy, especially at low frequencies.}
}

@article{falconperezSphericalMapsAcoustic2021,
  title = {Spherical {{Maps}} of {{Acoustic Properties}} as {{Feature Vectors}} in {{Machine-Learning-Based Estimation}} of {{Acoustic Parameters}}},
  author = {Falcón Pérez, Ricardo and Götz, Georg and Pulkki, Ville},
  date = {2021-09-10},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {J. Audio Eng. Soc.},
  volume = {69},
  number = {9},
  pages = {632--643},
  issn = {15494950},
  doi = {10.17743/jaes.2021.0011},
  url = {https://www.aes.org/e-lib/browse.cfm?elib=21460},
  urldate = {2025-03-16},
  langid = {english}
}

@article{falkNonIntrusiveQualityIntelligibility2010,
  title = {A {{Non-Intrusive Quality}} and {{Intelligibility Measure}} of {{Reverberant}} and {{Dereverberated Speech}}},
  author = {Falk, Tiago H. and Zheng, Chenxi and Chan, Wai-Yip},
  date = {2010-09},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE Trans. Audio Speech Lang. Process.},
  volume = {18},
  number = {7},
  pages = {1766--1774},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/tasl.2010.2052247},
  url = {http://ieeexplore.ieee.org/document/5547575/},
  urldate = {2025-07-20},
  abstract = {A modulation spectral representation is investigated for non-intrusive quality and intelligibility measurement of reverberant and dereverberated speech. The representation is obtained by means of an auditory-inspired filterbank analysis of criticalband temporal envelopes of the speech signal. Modulation spectral insights are used to develop an adaptive measure termed speech to reverberation modulation energy ratio. Experimental results show the proposed measure outperforming three standard algorithms for tasks involving estimation of multiple dimensions of perceived coloration, as well as quality measurement and intelligibility estimation of reverberant and dereverberated speech.},
  langid = {english}
}

@online{foundationBlenderorgHomeBlender,
  title = {Blender.Org - {{Home}} of the {{Blender}} Project - {{Free}} and {{Open 3D Creation Software}}},
  author = {Foundation, Blender},
  url = {https://www.blender.org/},
  urldate = {2025-07-19},
  abstract = {The Freedom to Create},
  langid = {english},
  organization = {blender.org}
}

@article{garcia-lazaroSensoryPerceptualDecisional2024,
  title = {Sensory and {{Perceptual Decisional Processes Underlying}} the {{Perception}} of {{Reverberant Auditory Environments}}},
  author = {García-Lázaro, Haydée G. and Teng, Santani},
  date = {2024-08-01},
  journaltitle = {eNeuro},
  shortjournal = {eNeuro},
  volume = {11},
  number = {8},
  eprint = {39122554},
  eprinttype = {pubmed},
  publisher = {Society for Neuroscience},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0122-24.2024},
  url = {https://www.eneuro.org/content/11/8/ENEURO.0122-24.2024},
  urldate = {2025-07-18},
  abstract = {Reverberation, a ubiquitous feature of real-world acoustic environments, exhibits statistical regularities that human listeners leverage to self-orient, facilitate auditory perception, and understand their environment. Despite the extensive research on sound source representation in the auditory system, it remains unclear how the brain represents real-world reverberant environments. Here, we characterized the neural response to reverberation of varying realism by applying multivariate pattern analysis to electroencephalographic (EEG) brain signals. Human listeners (12 males and 8 females) heard speech samples convolved with real-world and synthetic reverberant impulse responses and judged whether the speech samples were in a “real” or “fake” environment, focusing on the reverberant background rather than the properties of speech itself. Participants distinguished real from synthetic reverberation with ∼75\% accuracy; EEG decoding reveals a multistage decoding time course, with dissociable components early in the stimulus presentation and later in the perioffset stage. The early component predominantly occurred in temporal electrode clusters, while the later component was prominent in centroparietal clusters. These findings suggest distinct neural stages in perceiving natural acoustic environments, likely reflecting sensory encoding and higher-level perceptual decision-making processes. Overall, our findings provide evidence that reverberation, rather than being largely suppressed as a noise-like signal, carries relevant environmental information and gains representation along the auditory system. This understanding also offers various applications; it provides insights for including reverberation as a cue to aid navigation for blind and visually impaired people. It also helps to enhance realism perception in immersive virtual reality settings, gaming, music, and film production.},
  langid = {english},
  keywords = {auditory perception,EEG,MVPA,natural acoustic environments,reverberation}
}

@book{geronHandsonMachineLearning2023,
  title = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}} and {{TensorFlow}}: Concepts, Tools, and Techniques to Build Intelligent Systems},
  shorttitle = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}} and {{TensorFlow}}},
  author = {Géron, Aurélien},
  date = {2023},
  edition = {Third edition},
  publisher = {O'Reilly Media, Inc},
  location = {Beijing},
  abstract = {"Through a recent series of breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This best-selling book uses concrete examples, minimal theory, and production-ready Python frameworks--scikit-learn, Keras, and TensorFlow--to help you gain an intuitive understanding of the concepts and tools for building intelligent systems. With this updated third edition, author Aurelien Geron explores a range of techniques, starting with simple linear regression and progressing to deep neural networks. Numerous code examples and exercises throughout the book help you apply what you've learned. Programming experience is all you need to get started"--},
  isbn = {978-1-0981-2597-4},
  pagetotal = {834},
  keywords = {Apprentissage automatique,artificial intelligence,Artificial intelligence,Intelligence artificielle,Machine learning,Python (Computer program language),Python (Langage de programmation),TensorFlow}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  isbn = {978-0-262-03561-3},
  pagetotal = {775},
  keywords = {Machine learning}
}

@standard{internationalorganizationforstandardizationISO3382120092009,
  title = {{{ISO}} 3382-1:2009 {{Acoustics}} — {{Measurement}} of Room Acoustic Parameters},
  author = {{International Organization for Standardization}},
  date = {2009},
  number = {Part 1: Performance spaces},
  url = {https://www.iso.org/standard/40979.html}
}

@standard{ISO3382220082008,
  title = {{{ISO}} 3382-2:2008 {{Acoustics}} — {{Measurement}} of Room Acoustic Parameters},
  date = {2008},
  number = {Part 2: Reverberation time in ordinary rooms}
}

@article{jambrosicReverberationTimeMeasuring2008,
  title = {Reverberation Time Measuring Methods},
  author = {Jambrosic, Kristian and Horvat, Marko and Domitrovic, Hrvoje},
  date = {2008-05-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {123},
  pages = {3617--3617},
  publisher = {Acoustical Society of America (ASA)},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.2934829},
  url = {https://pubs.aip.org/jasa/article/123/5_Supplement/3617/715908/Reverberation-time-measuring-methods},
  urldate = {2025-07-18},
  abstract = {In this paper different well-established methods of reverberation time measurement are compared. Furthermore, the results obtained using these methods are compared to the results provided by some additional methods which could serve as an in situ tool if, for any reason, the reverberation time measurements cannot be carried out using the standardized methods. The methods compared in this paper include the standardized methods (EN ISO 3382:2000), namely the impulse response measured with pink noise, exponential sweep, MLS, but also pistol shots of different calibers, balloon bursts, gated external pink noise, and the B\&K filtered burst method. In order to make the comparison, the measurements were performed in four acoustically very different spaces - a rather small and well-damped listening room, a much bigger damped listening room, a rather reverberant atrium, and a large and very reverberant shoebox-shaped room. The results were evaluated according to signal-to-noise ratio criterion as well. Special attention has been given to the influence of room modes on measurement results.},
  issue = {5\_Supplement},
  langid = {english}
}

@book{kaplanArtificialIntelligenceWhat2016,
  title = {Artificial {{Intelligence}}: {{What Everyone Needs}} to {{Know}}®{{What Everyone Needs}} to {{Know}}®},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Kaplan, Jerry},
  date = {2016-11-24},
  doi = {10.1093/wentk/9780190602383.001.0001},
  abstract = {Over the coming decades, Artificial Intelligence will profoundly impact the way we live, work, wage war, play, seek a mate, educate our young, and care for our elderly. It is likely to greatly increase our aggregate wealth, but it will also upend our labor markets, reshuffle our social order, and strain our private and public institutions. Eventually it may alter how we see our place in the universe, as machines pursue goals independent of their creators and outperform us in domains previously believed to be the sole dominion of humans. Whether we regard them as conscious or unwitting, revere them as a new form of life or dismiss them as mere clever appliances, is beside the point. They are likely to play an increasingly critical and intimate role in many aspects of our lives. The emergence of systems capable of independent reasoning and action raises serious questions about just whose interests they are permitted to serve, and what limits our society should place on their creation and use. Deep ethical questions that have bedeviled philosophers for ages will suddenly arrive on the steps of our courthouses. Can a machine be held accountable for its actions? Should intelligent systems enjoy independent rights and responsibilities, or are they simple property? Who should be held responsible when a self-driving car kills a pedestrian? Can your personal robot hold your place in line, or be compelled to testify against you? If it turns out to be possible to upload your mind into a machine, is that still you? The answers may surprise you.},
  isbn = {978-0-19-060238-3}
}

@book{kapoorDeepLearningTensorFlow2019,
  title = {Deep {{Learning}} with {{TensorFlow}} 2 and {{Keras}}: {{Regression}}, {{ConvNets}}, {{GANs}}, {{RNNs}}, {{NLP}}, and More with {{TensorFlow}} 2 and the {{Keras API}}, 2nd {{Edition}}},
  shorttitle = {Deep {{Learning}} with {{TensorFlow}} 2 and {{Keras}}},
  author = {Kapoor, Amita and Guili, Antonio and Pal, Sujit},
  date = {2019-12-29},
  abstract = {Deep Learning with TensorFlow 2 and Keras, Second Edition teaches neural networks and deep learning techniques alongside TensorFlow (TF) and Keras. You'll learn how to write deep learning applications in the most powerful, popular, and scalable machine learning stack available. TensorFlow is the machine learning library of choice for professional applications, while Keras offers a simple and powerful Python API for accessing TensorFlow. TensorFlow 2 provides full Keras integration, making advanced machine learning easier and more convenient than ever before. This book also introduces neural networks with TensorFlow, runs through the main applications (regression, ConvNets (CNNs), GANs, RNNs, NLP), covers two working example apps, and then dives into TF in production, TF mobile, and using TensorFlow with AutoML.},
  isbn = {978-1-83882-341-2}
}

@article{kimAcousticRoomModelling2021,
  title = {Acoustic {{Room Modelling Using}} 360 {{Stereo Cameras}}},
  author = {Kim, Hansung and Remaggi, Luca and Fowler, Sam and Jackson, Philip Jb and Hilton, Adrian},
  date = {2021},
  journaltitle = {IEEE Transactions on Multimedia},
  shortjournal = {IEEE Trans. Multimedia},
  volume = {23},
  pages = {4117--4130},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/tmm.2020.3037537},
  url = {https://ieeexplore.ieee.org/document/9257197/},
  urldate = {2025-07-26},
  abstract = {In this paper we propose a pipeline for estimating acoustic 3D room structure with geometry and attribute prediction using spherical 360◦ cameras. Instead of setting microphone arrays with loudspeakers to measure acoustic parameters for specific rooms, a simple and practical single-shot capture of the scene using a stereo pair of 360 cameras can be used to simulate those acoustic parameters. We assume that the room and objects can be represented as cuboids aligned to the main axes of the room coordinate (Manhattan world). The scene is captured as a stereo pair using off-the-shelf consumer spherical 360 cameras. A cuboid-based 3D room geometry model is estimated by correspondence matching between captured images and semantic labelling using a convolutional neural network (SegNet). The estimated geometry is used to produce frequency-dependent acoustic predictions of the scene. This is, to our knowledge, the first attempt in the literature to use visual geometry estimation and object classification algorithms to predict acoustic properties. Results are compared to measurements through calculated reverberant spatial audio object parameters used for reverberation reproduction customized to the given loudspeaker set up.},
  langid = {english}
}

@book{kuttruffRoomAcoustics2006,
  title = {Room Acoustics},
  author = {Kuttruff, Heinrich},
  date = {2006},
  edition = {4. ed., transferred to digital printing},
  publisher = {Taylor \& Francis},
  location = {London New York},
  isbn = {978-0-419-24580-3},
  langid = {english},
  pagetotal = {349}
}

@article{lollmannImprovedAlgorithmBlind2010,
  title = {An Improved Algorithm for Blind Reverberation Time Estimation},
  author = {Löllmann, Heinrich and Yılmaz, Emre and Jeub, Marco and Vary, Peter},
  date = {2010-08-30},
  abstract = {An improved algorithm for the estimation of the reverberation time (RT) from reverberant speech signals is presented. This blind estimation of the RT is based on a simple statistical model for the sound decay such that the RT can be estimated by means of a maximum-likelihood (ML) estimator. The proposed algorithm has a significantly lower computational complexity than previous ML-based algorithms for RT estima-tion. This is achieved by a downsampling operation and a simple pre-selection of possible sound decays. The new algorithm is more suitable to track time-varying RTs than related approaches. In addition, it can also estimate the RT in the presence of (moderate) background noise. The proposed algorithm can be employed to measure the RT of rooms from sound recordings without using a dedicated measurement setup. Another possible application is its use within speech dereverberation systems for hands-free devices or digital hearing aids.}
}

@book{longArchitecturalAcoustics2006,
  title = {Architectural {{Acoustics}}},
  author = {Long, Marshall},
  date = {2006-01-01},
  url = {http://archive.org/details/ArchitecturalAcoustics_201901},
  urldate = {2025-07-27},
  abstract = {From the Applications of Modern Acoustics Series Edited by Moises Levy and Richard Stern},
  langid = {english},
  keywords = {architecture}
}

@article{lubeckBinauralReproductionMicrophone2025,
  title = {Binaural {{Reproduction}} of {{Microphone Array Recordings With 2D Video}} in {{Mixed Reality}}},
  author = {Lübeck, Tim and Ben-Hur, Zamir and Lou Alon, David and Crukley, Jeffery},
  date = {2025-07-07},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {J. Audio Eng. Soc.},
  volume = {73},
  number = {7/8},
  pages = {461--470},
  publisher = {Audio Engineering Society},
  issn = {1549-4950},
  doi = {10.17743/jaes.2022.0213},
  url = {https://aes2.org/publications/elibrary-page/?id=22924},
  urldate = {2025-07-18},
  abstract = {Head-worn devices equipped with microphone arrays and cameras can be used to capture the experience from a user’s perspective and reproduce it in virtual, mixed, or augmented reality. A concept that has recently introduced is to present the video capture as a 2D video screen augmented into the real-world environment through a mixed reality headset. This study presents such a system for reproducing audio and video capture from glasses arrays as a video “augment” along with binaural audio. Results of an initial listening experiment are presented, evaluating different state-of-the-art methods for binaural rendering. A stereo rendering through virtual loudspeakers attached to the video “augment” is compared with head-locked and world-locked binaural syntheses based on a binaural beamforming approach. The results suggest that listeners rated beamforming-based reproduction higher than stereo rendering. World-locked rendering was not rated significantly better than the head-locked version.},
  langid = {english}
}

@inreference{MeanAbsoluteError2025,
  title = {Mean Absolute Error},
  booktitle = {Wikipedia},
  date = {2025-02-16T18:40:23Z},
  url = {https://en.wikipedia.org/w/index.php?title=Mean_absolute_error&oldid=1276071917},
  urldate = {2025-07-22},
  abstract = {In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. MAE is calculated as the sum of absolute errors (i.e., the Manhattan distance) divided by the sample size:                                   M           A           E                  =                                                                 ∑                                    i                   =                   1                                                     n                                                                |                                                         y                                            i                                                           −                                        x                                            i                                                                          |                                         n                             =                                                                 ∑                                    i                   =                   1                                                     n                                                                |                                    e                                        i                                                     |                                         n                             .                 \{\textbackslash displaystyle \textbackslash mathrm \{MAE\} =\{\textbackslash frac \{\textbackslash sum \_\{i=1\}\textasciicircum\{n\}\textbackslash left|y\_\{i\}-x\_\{i\}\textbackslash right|\}\{n\}\}=\{\textbackslash frac \{\textbackslash sum \_\{i=1\}\textasciicircum\{n\}\textbackslash left|e\_\{i\}\textbackslash right|\}\{n\}\}.\}    It is thus an arithmetic average of the absolute errors                                    |                             e                        i                                        |                  =                    |                             y                        i                             −                    x                        i                                        |                          \{\textbackslash displaystyle |e\_\{i\}|=|y\_\{i\}-x\_\{i\}|\}    , where                                    y                        i                                     \{\textbackslash displaystyle y\_\{i\}\}     is the prediction and                                    x                        i                                     \{\textbackslash displaystyle x\_\{i\}\}     the true value. Alternative formulations may include relative frequencies as weight factors. The mean absolute error uses the same scale as the data being measured. This is known as a scale-dependent accuracy measure and therefore cannot be used to make comparisons between predicted values that use different scales. The mean absolute error is a common measure of forecast error in time series analysis, sometimes used in confusion with the more standard definition of mean absolute deviation. The same confusion exists more generally.},
  langid = {english},
  annotation = {Page Version ID: 1276071917}
}

@inreference{MeanSquaredError2025,
  title = {Mean Squared Error},
  booktitle = {Wikipedia},
  date = {2025-05-11T12:45:21Z},
  url = {https://en.wikipedia.org/w/index.php?title=Mean_squared_error&oldid=1289884489},
  urldate = {2025-07-22},
  abstract = {In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the true value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. In machine learning, specifically empirical risk minimization, MSE may refer to the empirical risk (the average loss on an observed data set), as an estimate of the true MSE (the true risk: the average loss on the actual population distribution). The MSE is a measure of the quality of an estimator.  As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero. The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the true value). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error.},
  langid = {english},
  annotation = {Page Version ID: 1289884489}
}

@inproceedings{milneUseArtificialIntelligence2020,
  title = {Use of {{Artificial Intelligence}} in {{Room Acoustics Prediction Using}} a {{Photograph}}},
  booktitle = {Reproduced {{Sound}} 2020},
  author = {Milne, D and Davison, L and Ausiello, L},
  date = {2020-12-04},
  publisher = {Institute of Acoustics},
  location = {Virtual},
  doi = {10.25144/13372},
  url = {https://www.ioa.org.uk/catalogue/paper/use-artificial-intelligence-room-acoustics-prediction-using-photograph},
  urldate = {2025-07-26},
  eventtitle = {Reproduced {{Sound}} 2020},
  langid = {english}
}

@inproceedings{nanniCombiningVisualAcoustic2016,
  title = {Combining {{Visual}} and {{Acoustic Features}} for {{Bird Species Classification}}},
  author = {Nanni, Loris and Costa, Yandre and Lucio, Diego and Silla, Carlos and Brahnam, Sheryl},
  date = {2016-11-01},
  pages = {396--401},
  doi = {10.1109/ICTAI.2016.0067}
}

@online{owensVisuallyIndicatedSounds2016,
  title = {Visually {{Indicated Sounds}}},
  author = {Owens, Andrew and Isola, Phillip and McDermott, Josh and Torralba, Antonio and Adelson, Edward H. and Freeman, William T.},
  date = {2016-04-30},
  eprint = {1512.08512},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.08512},
  url = {http://arxiv.org/abs/1512.08512},
  urldate = {2025-07-26},
  abstract = {Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a "real or fake" psychophysical experiment, and that they convey significant information about material properties and physical interactions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound}
}

@article{perezMachinelearningbasedEstimationReverberation,
  title = {Machine-Learning-Based Estimation of Reverberation Time Using Room Geometry for Room Effect Rendering},
  author = {Pérez, Ricardo FALCÓN and Götz, Georg and Pulkki, Ville},
  abstract = {This work presents a machine-learning-based method to estimate the reverberation time of a virtual room for auralization purposes. The models take as input geometric features of the room and output the estimated reverberation time values as function of frequency. The proposed model is trained and evaluated using a novel dataset composed of real-world acoustical measurements of a single room with 832 different configurations of furniture and absorptive materials, for multiple loudspeaker positions. The method achieves a prediction accuracy of approximately 90\% for most frequency bands. Furthermore, when comparing against the Sabine and Eyring methods, the proposed approach exhibits a much higher accuracy, especially at low frequencies.},
  langid = {english}
}

@article{pratesBlindEstimationReverberation,
  title = {Blind Estimation of Reverberation Time by Neural Networks},
  author = {Prates, Rodrigo L and Petraglia, Mariane R and Torres, Julio C B and Petraglia, Antonio},
  abstract = {In this paper we investigate a procedure for blind estimation of room reverberation time, that is, without a priori knowledge of the room impulse response. A neural network based method is proposed, which is robust to noise and to abrupt variations over time of the acquired signal. The input features of the neural network are calculated in the frequency domain, where training samples are generated in the Mel scale. A competent network architecture (i.e., with appropriate number and type of hidden layers and number of neurons comprising each layer), as well as the best training algorithm and regularization technique, are investigated. The use of cross-entropy error during training gives better results than those obtained by mean squared error. Comparative results are presented, considering two previously proposed blind methods: a deep neural network technique and a spectral decay distribution method.},
  langid = {english}
}

@book{pregoBlindEstimatorsReverberation2015,
  title = {Blind Estimators for Reverberation Time and Direct-to-Reverberant Energy Ratio Using Subband Speech Decomposition},
  author = {Prego, Thiago and Lima, Amaro and Zambrano-Lopez, Rafael and Netto, Sergio},
  date = {2015-10-01},
  pages = {5},
  doi = {10.1109/WASPAA.2015.7336954},
  pagetotal = {1}
}

@software{PyfarPyfar2025,
  title = {Pyfar/Pyfar},
  date = {2025-07-04T13:02:59Z},
  origdate = {2020-10-27T19:13:03Z},
  url = {https://github.com/pyfar/pyfar},
  urldate = {2025-07-19},
  abstract = {python package for acoustics research},
  organization = {pyfar},
  keywords = {audio-in-out,digital-signal-processing,plotting}
}

@software{PyfarPyrato2025,
  title = {Pyfar/Pyrato},
  date = {2025-06-30T13:11:13Z},
  origdate = {2020-09-10T14:58:47Z},
  url = {https://github.com/pyfar/pyrato},
  urldate = {2025-07-19},
  abstract = {Python Room Acoustics Tools},
  organization = {pyfar}
}

@article{ratnamBlindEstimationReverberation2003,
  title = {Blind Estimation of Reverberation Time},
  author = {Ratnam, Rama and Jones, Douglas L. and Wheeler, Bruce C. and O’Brien, William D. and Lansing, Charissa R. and Feng, Albert S.},
  date = {2003-11-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {114},
  number = {5},
  pages = {2877--2892},
  publisher = {Acoustical Society of America (ASA)},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.1616578},
  url = {https://pubs.aip.org/jasa/article/114/5/2877/547709/Blind-estimation-of-reverberation-time},
  urldate = {2025-07-20},
  abstract = {The reverberation time (RT) is an important parameter for characterizing the quality of an auditory space. Sounds in reverberant environments are subject to coloration. This affects speech intelligibility and sound localization. Many state-of-the-art audio signal processing algorithms, for example in hearing-aids and telephony, are expected to have the ability to characterize the listening environment, and turn on an appropriate processing strategy accordingly. Thus, a method for characterization of room RT based on passively received microphone signals represents an important enabling technology. Current RT estimators, such as Schroeder’s method, depend on a controlled sound source, and thus cannot produce an online, blind RT estimate. Here, a method for estimating RT without prior knowledge of sound sources or room geometry is presented. The diffusive tail of reverberation was modeled as an exponentially damped Gaussian white noise process. The time-constant of the decay, which provided a measure of the RT, was estimated using a maximum-likelihood procedure. The estimates were obtained continuously, and an order-statistics filter was used to extract the most likely RT from the accumulated estimates. The procedure was illustrated for connected speech. Results obtained for simulated and real room data are in good agreement with the real RT values.},
  langid = {english}
}

@online{ratnarajahAVRIRAudioVisualRoom2024,
  title = {{{AV-RIR}}: {{Audio-Visual Room Impulse Response Estimation}}},
  shorttitle = {{{AV-RIR}}},
  author = {Ratnarajah, Anton and Ghosh, Sreyan and Kumar, Sonal and Chiniya, Purva and Manocha, Dinesh},
  date = {2024-04-23},
  eprint = {2312.00834},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.00834},
  url = {http://arxiv.org/abs/2312.00834},
  urldate = {2025-07-26},
  abstract = {Accurate estimation of Room Impulse Response (RIR), which captures an environment's acoustic properties, is important for speech processing and AR/VR applications. We propose AV-RIR, a novel multi-modal multi-task learning approach to accurately estimate the RIR from a given reverberant speech signal and the visual cues of its corresponding environment. AV-RIR builds on a novel neural codec-based architecture that effectively captures environment geometry and materials properties and solves speech dereverberation as an auxiliary task by using multi-task learning. We also propose Geo-Mat features that augment material information into visual cues and CRIP that improves late reverberation components in the estimated RIR via image-to-RIR retrieval by 86\%. Empirical results show that AV-RIR quantitatively outperforms previous audio-only and visual-only approaches by achieving 36\% - 63\% improvement across various acoustic metrics in RIR estimation. Additionally, it also achieves higher preference scores in human evaluation. As an auxiliary benefit, dereverbed speech from AV-RIR shows competitive performance with the state-of-the-art in various spoken language processing tasks and outperforms reverberation time error score in the real-world AVSpeech dataset. Qualitative examples of both synthesized reverberant speech and enhanced speech can be found at https://www.youtube.com/watch?v=tTsKhviukAE.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Sound}
}

@online{richterEARSAnechoicFullband2024,
  title = {{{EARS}}: {{An Anechoic Fullband Speech Dataset Benchmarked}} for {{Speech Enhancement}} and {{Dereverberation}}},
  shorttitle = {{{EARS}}},
  author = {Richter, Julius and Wu, Yi-Chiao and Krenn, Steven and Welker, Simon and Lay, Bunlong and Watanabe, Shinji and Richard, Alexander and Gerkmann, Timo},
  date = {2024-06-11},
  eprint = {2406.06185},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2406.06185},
  url = {http://arxiv.org/abs/2406.06185},
  urldate = {2025-07-19},
  abstract = {We release the EARS (Expressive Anechoic Recordings of Speech) dataset, a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totaling in 100 hours of clean, anechoic speech data. The dataset covers a large range of different speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech. We benchmark various methods for speech enhancement and dereverberation on the dataset and evaluate their performance through a set of instrumental metrics. In addition, we conduct a listening test with 20 participants for the speech enhancement task, where a generative method is preferred. We introduce a blind test set that allows for automatic online evaluation of uploaded data. Dataset download links and automatic evaluation server can be found online1.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@online{RoomImpulseResponse,
  title = {Room Impulse Response Simulation with Stochastic Ray Tracing - MATLAB \&amp; Simulink},
  url = {https://www.mathworks.com/help/audio/ug/room-impulse-response-simulation-with-stochastic-ray-tracing.html},
  urldate = {2025-07-16},
  abstract = {Use stochastic ray tracing to simulate the impulse response of a simple room.},
  langid = {ngerman}
}

@online{RoomImpulseResponsea,
  title = {Room Impulse Response Simulation with the Image-Source Method and HRTF Interpolation - MATLAB \&amp; Simulink},
  url = {https://www.mathworks.com/help/audio/ug/room-impulse-response-simulation-with-image-source-method-and-hrtf-interpolation.html},
  urldate = {2025-07-16},
  abstract = {Simulate the impulse response of a "shoebox" (cuboid) empty room.},
  langid = {ngerman}
}

@book{sabineCollectedPapersAcoustics1922,
  title = {Collected Papers on Acoustics},
  author = {Sabine, Wallace Clement},
  namea = {{University of California Libraries}},
  nameatype = {collaborator},
  date = {1922},
  publisher = {Cambridge : Harvard University Press},
  url = {http://archive.org/details/collectedpaperso00sabi},
  urldate = {2025-07-16},
  abstract = {ix, 279 p. 27 cm},
  langid = {english},
  pagetotal = {308},
  keywords = {Architectural acoustics}
}

@inbook{sabineReverberation1922,
  title = {Reverberation},
  booktitle = {Collected Papers on Acoustics},
  namea = {{University of California Libraries}},
  nameatype = {collaborator},
  date = {1922},
  pages = {3--68},
  publisher = {Cambridge : Harvard University Press},
  url = {http://archive.org/details/collectedpaperso00sabi},
  urldate = {2025-07-16},
  bookauthor = {Sabine, Wallace Clement},
  langid = {english},
  keywords = {Architectural acoustics}
}

@article{schaabDemonstratorAuralizationControl2017,
  title = {Demonstrator for the Auralization and Control of the Room Divergence Effect},
  author = {Schaab, M and Dobmeier, V and Werner, S and Klein, F},
  date = {2017},
  abstract = {The goal of binaural headphone reproduction is to synthesize a virtual room or to resynthesize the acoustics of a real room. Former research has shown, that the acoustical divergence between the room presented over headphones and the actual listening room can violate the expectations of the listener. In this case, the perceived quality of the synthesized room is degraded despite of a technical correct synthesis of the ear signals. This effect is called room divergence effect and is measured in a reduction of externalization of sound events. This publication describes a demonstrator which auralizes this effect. For this purpose a 5 channel loudspeaker setup is measured with a KEMAR artificial head in two rooms. Additionally three algorithms are implemented to calculate virtual rooms in between the measured rooms. By listening to the unmodified rooms measurements and their modifications differences in externalization are distinguishable. The influence of each algorithm on externalization in a divergent listening scenario is evaluated in a listening test with 14 participants.},
  langid = {english}
}

@article{selvarajuGradCAMVisualExplanations2016,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2016},
  doi = {10.48550/ARXIV.1610.02391},
  url = {https://arxiv.org/abs/1610.02391},
  urldate = {2025-05-23},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.}
}

@article{shelleyOpenAIR129thAudio2010,
  title = {{{OpenAIR}}: 129th {{Audio Engineering Society Convention}} 2010},
  shorttitle = {{{OpenAIR}}},
  author = {Shelley, Simon and Murphy, Damian T.},
  date = {2010},
  journaltitle = {129th Audio Engineering Society Convention 2010},
  series = {129th {{Audio Engineering Society Convention}} 2010},
  pages = {1270--1278},
  issn = {9781617821943},
  url = {http://www.scopus.com/inward/record.url?scp=84866013022&partnerID=8YFLogxK},
  urldate = {2025-07-19},
  abstract = {There have been many recent initiatives to capture the impulse responses of important or interesting acoustic spaces, although not all of this data has been made more widely available to researchers interested in auralization. This paper presents the Open Acoustic Impulse Response (OpenAIR) Library, a new online resource allowing users to share impulse responses and related acoustical information. Open-source software is provided, enabling the user to render the acoustical data using various auralization strategies. Software tools and guidelines for the process of impulse response capture are also provided, aiming to disseminate best practice. The database can accommodate impulse response datasets captured according to different measurement techniques and the use of robust spatial audio coding formats is also considered for the distribution of this type of information. Visitors to the resource can search for acoustical data using keywords, and can also browse uploaded datasets on a world map.}
}

@article{shinn-cunninghamBarbaraShinnCunningham2015,
  title = {Barbara {{Shinn-Cunningham}}},
  author = {Shinn-Cunningham, Barbara},
  date = {2015-06},
  journaltitle = {Current Biology},
  volume = {25},
  number = {11},
  pages = {R442-R444},
  publisher = {Elsevier BV},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2015.02.060},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982215002298},
  urldate = {2025-07-18},
  abstract = {One aspect of hearing that has received relatively little attention by traditional psychophysicists is how echoes and reverberation in everyday spaces affect perception. In the ordinary world, echoes and reverberation are ubiquitous and influence the signals reaching the listener, the processing of these signals by the brain, and the resulting perception of both sound sources and the environment. Many aspects of the signals reaching the ear are altered or "distorted" by echoes and reverberation, including spectral content, interaural differences, and temporal structure. As a result, echoes and reverberation could influence many aspects of perception, including spatial hearing in direction and distance, speech intelligibility, and spatial unmasking. This paper reviews results from a number of studies examining how the acoustics of ordinary rooms affect various aspects of the signals reaching a listener's ears as well as resulting perception. While the acoustic effects of reverberant energy are often pronounced, performance on most behavioral tasks is relatively robust to these effects. These perceptual results suggest that listeners may not simply be adept at ignoring the signal distortion caused by ordinary room acoustics, but may be adapted to deal with its presence. These results are important for designing truly immersive spatial auditory displays, because they demonstrate the importance of reverberant energy for achieving a realistic, immersive experience.},
  langid = {english}
}

@online{singhImage2ReverbCrossModalReverb2021,
  title = {{{Image2Reverb}}: {{Cross-Modal Reverb Impulse Response Synthesis}}},
  shorttitle = {{{Image2Reverb}}},
  author = {Singh, Nikhil and Mentch, Jeff and Ng, Jerry and Beveridge, Matthew and Drori, Iddo},
  date = {2021-08-13},
  eprint = {2103.14201},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.14201},
  url = {http://arxiv.org/abs/2103.14201},
  urldate = {2025-07-19},
  abstract = {Measuring the acoustic characteristics of a space is often done by capturing its impulse response (IR), a representation of how a full-range stimulus sound excites it. This work generates an IR from a single image, which can then be applied to other signals using convolution, simulating the reverberant characteristics of the space shown in the image. Recording these IRs is both time-intensive and expensive, and often infeasible for inaccessible locations. We use an end-to-end neural network architecture to generate plausible audio impulse responses from single images of acoustic environments. We evaluate our method both by comparisons to ground truth data and by human expert evaluation. We demonstrate our approach by generating plausible impulse responses from diverse settings and formats including well known places, musical halls, rooms in paintings, images from animations and computer games, synthetic environments generated from text, panoramic images, and video conference backgrounds.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@online{Treble,
  title = {Treble},
  url = {https://www.treble.tech/},
  urldate = {2025-07-19}
}

@inproceedings{veauxVoiceBankCorpus2013,
  title = {The Voice Bank Corpus: {{Design}}, Collection and Data Analysis of a Large Regional Accent Speech Database},
  shorttitle = {The Voice Bank Corpus},
  booktitle = {2013 {{International Conference Oriental COCOSDA}} Held Jointly with 2013 {{Conference}} on {{Asian Spoken Language Research}} and {{Evaluation}} ({{O-COCOSDA}}/{{CASLRE}})},
  author = {Veaux, Christophe and Yamagishi, Junichi and King, Simon},
  date = {2013-11},
  pages = {1--4},
  doi = {10.1109/ICSDA.2013.6709856},
  url = {https://ieeexplore.ieee.org/document/6709856},
  urldate = {2025-07-19},
  abstract = {The University of Edinburgh has started the development of a new speech database, the Voice Bank corpus, specifically designed for the creation of personalised synthetic voices for individuals with speech disorders. This corpus already constitutes the largest corpora of British English currently in existence, with more than 300 hours of recordings from approximately 500 healthy speakers. New recordings are continuously being made in order to get the best coverage of the different combinations of regional accents, social classes, age and gender across Britain. This paper describes the motivation and the processes involved in the design and recording of this corpus as well as some analysis of its content. The paper concludes with our future plans to further extend this corpus and to overcome its current limitations.},
  eventtitle = {2013 {{International Conference Oriental COCOSDA}} Held Jointly with 2013 {{Conference}} on {{Asian Spoken Language Research}} and {{Evaluation}} ({{O-COCOSDA}}/{{CASLRE}})},
  keywords = {Corpus Design,Databases,Educational institutions,Hidden Markov models,Optimization,Recruitment,Speech,Speech synthesis,Speech Synthesis,Text Selection,Voice Banking}
}

@inproceedings{witmayerAutomatingMetadataLogging2018,
  title = {Automating {{Metadata Logging}} through {{Artificial Intelligence}}},
  author = {Witmayer, Christopher},
  date = {2018-10-01},
  pages = {1--10},
  doi = {10.5594/M001814}
}

@article{wittebolHybridRoomAcoustic2024,
  title = {A Hybrid Room Acoustic Modeling Approach Combining Image Source, Acoustic Diffusion Equation, and Time-Domain Discontinuous {{Galerkin}} Methods},
  author = {Wittebol, Wouter and Wang, Huiqing and Hornikx, Maarten and Calamia, Paul},
  date = {2024-07-05},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {223},
  pages = {110068},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2024.110068},
  url = {https://www.sciencedirect.com/science/article/pii/S0003682X24002196},
  urldate = {2025-07-16},
  abstract = {In this paper a hybrid model is introduced that constructs a broadband room impulse response using a geometrical (image source method) and a statistical method (acoustic diffusion equation) for the high-frequency range, supported by a wave-based method (time-domain discontinuous Galerkin method) for the low-frequency range. A crucial element concerns the construction of the high-frequency impulse response where a transition from a predominantly specular (image source) to a predominantly diffuse sound-field (diffusion equation) is required. To achieve this transition an analytical envelope is introduced. A key factor is the room-averaged scattering coefficient which accounts for all scattering behavior of the room and determines the speed of transition from a specular to a non-specular sound-field. To evaluate its performance, the model is compared to a broadband wave-based solver for two reference scenarios. The hybrid model shows promising results in terms of reverberation time (T20), center time (Ts) and bass-ratio (BR). Aspects such as the used geometrical complexity, the ‘room-averaged’ scattering coefficients, and other model simplifications and assumptions are discussed.},
  keywords = {Acoustic diffusion equation method,Hybrid,Image source method,Room acoustics,Time-domain discontinuous Galerkin method}
}

@online{yangDepthAnythingUnleashing2024,
  title = {Depth {{Anything}}: {{Unleashing}} the {{Power}} of {{Large-Scale Unlabeled Data}}},
  shorttitle = {Depth {{Anything}}},
  author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  date = {2024-04-07},
  eprint = {2401.10891},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.10891},
  url = {http://arxiv.org/abs/2401.10891},
  urldate = {2025-07-19},
  abstract = {This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (\textasciitilde 62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@book{zotterAmbisonicsPractical3D2019,
  title = {Ambisonics: {{A Practical 3D Audio Theory}} for {{Recording}}, {{Studio Production}}, {{Sound Reinforcement}}, and {{Virtual Reality}}},
  shorttitle = {Ambisonics},
  author = {Zotter, Franz and Frank, Matthias},
  date = {2019},
  series = {Springer {{Topics}} in {{Signal Processing}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  issn = {1866-2609, 1866-2617},
  doi = {10.1007/978-3-030-17207-7},
  url = {http://link.springer.com/10.1007/978-3-030-17207-7},
  urldate = {2025-07-18},
  isbn = {978-3-030-17206-0 978-3-030-17207-7},
  langid = {english},
  keywords = {Ambisonics Book,Binaural Listening,Imaginary Loudspeakers,Open Access,Playback Technology,Psychoacoustics,Scene-based Sound,Spherical Array Processing,Surround Sound Recording}
}
