@article{eyringREVERBERATIONTIMEDEAD1930,
  title = {{{REVERBERATION TIME IN}} “{{DEAD}}” {{ROOMS}}},
  author = {Eyring, Carl F.},
  date = {1930-01-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {1},
  pages = {217--241},
  issn = {0001-4966},
  doi = {10.1121/1.1915175},
  url = {https://doi.org/10.1121/1.1915175},
  urldate = {2025-07-16},
  issue = {2A}
}

@article{falconperezSphericalMapsAcoustic2021,
  title = {Spherical {{Maps}} of {{Acoustic Properties}} as {{Feature Vectors}} in {{Machine-Learning-Based Estimation}} of {{Acoustic Parameters}}},
  author = {Falcón Pérez, Ricardo and Götz, Georg and Pulkki, Ville},
  date = {2021-09-10},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {J. Audio Eng. Soc.},
  volume = {69},
  number = {9},
  pages = {632--643},
  issn = {15494950},
  doi = {10.17743/jaes.2021.0011},
  url = {https://www.aes.org/e-lib/browse.cfm?elib=21460},
  urldate = {2025-03-16},
  langid = {english}
}

@online{foundationBlenderorgHomeBlender,
  title = {Blender.Org - {{Home}} of the {{Blender}} Project - {{Free}} and {{Open 3D Creation Software}}},
  author = {Foundation, Blender},
  url = {https://www.blender.org/},
  urldate = {2025-07-19},
  abstract = {The Freedom to Create},
  langid = {english},
  organization = {blender.org}
}

@article{garcia-lazaroSensoryPerceptualDecisional2024,
  title = {Sensory and {{Perceptual Decisional Processes Underlying}} the {{Perception}} of {{Reverberant Auditory Environments}}},
  author = {García-Lázaro, Haydée G. and Teng, Santani},
  date = {2024-08-01},
  journaltitle = {eNeuro},
  shortjournal = {eNeuro},
  volume = {11},
  number = {8},
  eprint = {39122554},
  eprinttype = {pubmed},
  publisher = {Society for Neuroscience},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0122-24.2024},
  url = {https://www.eneuro.org/content/11/8/ENEURO.0122-24.2024},
  urldate = {2025-07-18},
  abstract = {Reverberation, a ubiquitous feature of real-world acoustic environments, exhibits statistical regularities that human listeners leverage to self-orient, facilitate auditory perception, and understand their environment. Despite the extensive research on sound source representation in the auditory system, it remains unclear how the brain represents real-world reverberant environments. Here, we characterized the neural response to reverberation of varying realism by applying multivariate pattern analysis to electroencephalographic (EEG) brain signals. Human listeners (12 males and 8 females) heard speech samples convolved with real-world and synthetic reverberant impulse responses and judged whether the speech samples were in a “real” or “fake” environment, focusing on the reverberant background rather than the properties of speech itself. Participants distinguished real from synthetic reverberation with ∼75\% accuracy; EEG decoding reveals a multistage decoding time course, with dissociable components early in the stimulus presentation and later in the perioffset stage. The early component predominantly occurred in temporal electrode clusters, while the later component was prominent in centroparietal clusters. These findings suggest distinct neural stages in perceiving natural acoustic environments, likely reflecting sensory encoding and higher-level perceptual decision-making processes. Overall, our findings provide evidence that reverberation, rather than being largely suppressed as a noise-like signal, carries relevant environmental information and gains representation along the auditory system. This understanding also offers various applications; it provides insights for including reverberation as a cue to aid navigation for blind and visually impaired people. It also helps to enhance realism perception in immersive virtual reality settings, gaming, music, and film production.},
  langid = {english},
  keywords = {auditory perception,EEG,MVPA,natural acoustic environments,reverberation}
}

@article{jambrosicReverberationTimeMeasuring2008,
  title = {Reverberation Time Measuring Methods},
  author = {Jambrosic, Kristian and Horvat, Marko and Domitrovic, Hrvoje},
  date = {2008-05-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {123},
  pages = {3617--3617},
  publisher = {Acoustical Society of America (ASA)},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.2934829},
  url = {https://pubs.aip.org/jasa/article/123/5_Supplement/3617/715908/Reverberation-time-measuring-methods},
  urldate = {2025-07-18},
  abstract = {In this paper different well-established methods of reverberation time measurement are compared. Furthermore, the results obtained using these methods are compared to the results provided by some additional methods which could serve as an in situ tool if, for any reason, the reverberation time measurements cannot be carried out using the standardized methods. The methods compared in this paper include the standardized methods (EN ISO 3382:2000), namely the impulse response measured with pink noise, exponential sweep, MLS, but also pistol shots of different calibers, balloon bursts, gated external pink noise, and the B\&K filtered burst method. In order to make the comparison, the measurements were performed in four acoustically very different spaces - a rather small and well-damped listening room, a much bigger damped listening room, a rather reverberant atrium, and a large and very reverberant shoebox-shaped room. The results were evaluated according to signal-to-noise ratio criterion as well. Special attention has been given to the influence of room modes on measurement results.},
  issue = {5\_Supplement},
  langid = {english}
}

@article{lubeckBinauralReproductionMicrophone2025,
  title = {Binaural {{Reproduction}} of {{Microphone Array Recordings With 2D Video}} in {{Mixed Reality}}},
  author = {Lübeck, Tim and Ben-Hur, Zamir and Lou Alon, David and Crukley, Jeffery},
  date = {2025-07-07},
  journaltitle = {Journal of the Audio Engineering Society},
  shortjournal = {J. Audio Eng. Soc.},
  volume = {73},
  number = {7/8},
  pages = {461--470},
  publisher = {Audio Engineering Society},
  issn = {1549-4950},
  doi = {10.17743/jaes.2022.0213},
  url = {https://aes2.org/publications/elibrary-page/?id=22924},
  urldate = {2025-07-18},
  abstract = {Head-worn devices equipped with microphone arrays and cameras can be used to capture the experience from a user’s perspective and reproduce it in virtual, mixed, or augmented reality. A concept that has recently introduced is to present the video capture as a 2D video screen augmented into the real-world environment through a mixed reality headset. This study presents such a system for reproducing audio and video capture from glasses arrays as a video “augment” along with binaural audio. Results of an initial listening experiment are presented, evaluating different state-of-the-art methods for binaural rendering. A stereo rendering through virtual loudspeakers attached to the video “augment” is compared with head-locked and world-locked binaural syntheses based on a binaural beamforming approach. The results suggest that listeners rated beamforming-based reproduction higher than stereo rendering. World-locked rendering was not rated significantly better than the head-locked version.},
  langid = {english}
}

@article{perezMachinelearningbasedEstimationReverberation,
  title = {Machine-Learning-Based Estimation of Reverberation Time Using Room Geometry for Room Effect Rendering},
  author = {Pérez, Ricardo FALCÓN and Götz, Georg and Pulkki, Ville},
  abstract = {This work presents a machine-learning-based method to estimate the reverberation time of a virtual room for auralization purposes. The models take as input geometric features of the room and output the estimated reverberation time values as function of frequency. The proposed model is trained and evaluated using a novel dataset composed of real-world acoustical measurements of a single room with 832 different configurations of furniture and absorptive materials, for multiple loudspeaker positions. The method achieves a prediction accuracy of approximately 90\% for most frequency bands. Furthermore, when comparing against the Sabine and Eyring methods, the proposed approach exhibits a much higher accuracy, especially at low frequencies.},
  langid = {english}
}

@software{PyfarPyfar2025,
  title = {Pyfar/Pyfar},
  date = {2025-07-04T13:02:59Z},
  origdate = {2020-10-27T19:13:03Z},
  url = {https://github.com/pyfar/pyfar},
  urldate = {2025-07-19},
  abstract = {python package for acoustics research},
  organization = {pyfar},
  keywords = {audio-in-out,digital-signal-processing,plotting}
}

@software{PyfarPyrato2025,
  title = {Pyfar/Pyrato},
  date = {2025-06-30T13:11:13Z},
  origdate = {2020-09-10T14:58:47Z},
  url = {https://github.com/pyfar/pyrato},
  urldate = {2025-07-19},
  abstract = {Python Room Acoustics Tools},
  organization = {pyfar}
}

@online{richterEARSAnechoicFullband2024,
  title = {{{EARS}}: {{An Anechoic Fullband Speech Dataset Benchmarked}} for {{Speech Enhancement}} and {{Dereverberation}}},
  shorttitle = {{{EARS}}},
  author = {Richter, Julius and Wu, Yi-Chiao and Krenn, Steven and Welker, Simon and Lay, Bunlong and Watanabe, Shinji and Richard, Alexander and Gerkmann, Timo},
  date = {2024-06-11},
  eprint = {2406.06185},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2406.06185},
  url = {http://arxiv.org/abs/2406.06185},
  urldate = {2025-07-19},
  abstract = {We release the EARS (Expressive Anechoic Recordings of Speech) dataset, a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totaling in 100 hours of clean, anechoic speech data. The dataset covers a large range of different speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech. We benchmark various methods for speech enhancement and dereverberation on the dataset and evaluate their performance through a set of instrumental metrics. In addition, we conduct a listening test with 20 participants for the speech enhancement task, where a generative method is preferred. We introduce a blind test set that allows for automatic online evaluation of uploaded data. Dataset download links and automatic evaluation server can be found online1.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@online{RoomImpulseResponse,
  title = {Room Impulse Response Simulation with Stochastic Ray Tracing - MATLAB \&amp; Simulink},
  url = {https://www.mathworks.com/help/audio/ug/room-impulse-response-simulation-with-stochastic-ray-tracing.html},
  urldate = {2025-07-16},
  abstract = {Use stochastic ray tracing to simulate the impulse response of a simple room.},
  langid = {ngerman}
}

@online{RoomImpulseResponsea,
  title = {Room Impulse Response Simulation with the Image-Source Method and HRTF Interpolation - MATLAB \&amp; Simulink},
  url = {https://www.mathworks.com/help/audio/ug/room-impulse-response-simulation-with-image-source-method-and-hrtf-interpolation.html},
  urldate = {2025-07-16},
  abstract = {Simulate the impulse response of a "shoebox" (cuboid) empty room.},
  langid = {ngerman}
}

@book{sabineCollectedPapersAcoustics1922,
  title = {Collected Papers on Acoustics},
  author = {Sabine, Wallace Clement},
  namea = {{University of California Libraries}},
  nameatype = {collaborator},
  date = {1922},
  publisher = {Cambridge : Harvard University Press},
  url = {http://archive.org/details/collectedpaperso00sabi},
  urldate = {2025-07-16},
  abstract = {ix, 279 p. 27 cm},
  langid = {english},
  pagetotal = {308},
  keywords = {Architectural acoustics}
}

@inbook{sabineReverberation1922,
  title = {Reverberation},
  booktitle = {Collected Papers on Acoustics},
  namea = {{University of California Libraries}},
  nameatype = {collaborator},
  date = {1922},
  pages = {3--68},
  publisher = {Cambridge : Harvard University Press},
  url = {http://archive.org/details/collectedpaperso00sabi},
  urldate = {2025-07-16},
  bookauthor = {Sabine, Wallace Clement},
  langid = {english},
  keywords = {Architectural acoustics}
}

@article{schaabDemonstratorAuralizationControl2017,
  title = {Demonstrator for the Auralization and Control of the Room Divergence Effect},
  author = {Schaab, M and Dobmeier, V and Werner, S and Klein, F},
  date = {2017},
  abstract = {The goal of binaural headphone reproduction is to synthesize a virtual room or to resynthesize the acoustics of a real room. Former research has shown, that the acoustical divergence between the room presented over headphones and the actual listening room can violate the expectations of the listener. In this case, the perceived quality of the synthesized room is degraded despite of a technical correct synthesis of the ear signals. This effect is called room divergence effect and is measured in a reduction of externalization of sound events. This publication describes a demonstrator which auralizes this effect. For this purpose a 5 channel loudspeaker setup is measured with a KEMAR artificial head in two rooms. Additionally three algorithms are implemented to calculate virtual rooms in between the measured rooms. By listening to the unmodified rooms measurements and their modifications differences in externalization are distinguishable. The influence of each algorithm on externalization in a divergent listening scenario is evaluated in a listening test with 14 participants.},
  langid = {english}
}

@article{selvarajuGradCAMVisualExplanations2016,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2016},
  doi = {10.48550/ARXIV.1610.02391},
  url = {https://arxiv.org/abs/1610.02391},
  urldate = {2025-05-23},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.}
}

@article{shelleyOpenAIR129thAudio2010,
  title = {{{OpenAIR}}: 129th {{Audio Engineering Society Convention}} 2010},
  shorttitle = {{{OpenAIR}}},
  author = {Shelley, Simon and Murphy, Damian T.},
  date = {2010},
  journaltitle = {129th Audio Engineering Society Convention 2010},
  series = {129th {{Audio Engineering Society Convention}} 2010},
  pages = {1270--1278},
  issn = {9781617821943},
  url = {http://www.scopus.com/inward/record.url?scp=84866013022&partnerID=8YFLogxK},
  urldate = {2025-07-19},
  abstract = {There have been many recent initiatives to capture the impulse responses of important or interesting acoustic spaces, although not all of this data has been made more widely available to researchers interested in auralization. This paper presents the Open Acoustic Impulse Response (OpenAIR) Library, a new online resource allowing users to share impulse responses and related acoustical information. Open-source software is provided, enabling the user to render the acoustical data using various auralization strategies. Software tools and guidelines for the process of impulse response capture are also provided, aiming to disseminate best practice. The database can accommodate impulse response datasets captured according to different measurement techniques and the use of robust spatial audio coding formats is also considered for the distribution of this type of information. Visitors to the resource can search for acoustical data using keywords, and can also browse uploaded datasets on a world map.}
}

@article{shinn-cunninghamBarbaraShinnCunningham2015,
  title = {Barbara {{Shinn-Cunningham}}},
  author = {Shinn-Cunningham, Barbara},
  date = {2015-06},
  journaltitle = {Current Biology},
  volume = {25},
  number = {11},
  pages = {R442-R444},
  publisher = {Elsevier BV},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2015.02.060},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982215002298},
  urldate = {2025-07-18},
  abstract = {One aspect of hearing that has received relatively little attention by traditional psychophysicists is how echoes and reverberation in everyday spaces affect perception. In the ordinary world, echoes and reverberation are ubiquitous and influence the signals reaching the listener, the processing of these signals by the brain, and the resulting perception of both sound sources and the environment. Many aspects of the signals reaching the ear are altered or "distorted" by echoes and reverberation, including spectral content, interaural differences, and temporal structure. As a result, echoes and reverberation could influence many aspects of perception, including spatial hearing in direction and distance, speech intelligibility, and spatial unmasking. This paper reviews results from a number of studies examining how the acoustics of ordinary rooms affect various aspects of the signals reaching a listener's ears as well as resulting perception. While the acoustic effects of reverberant energy are often pronounced, performance on most behavioral tasks is relatively robust to these effects. These perceptual results suggest that listeners may not simply be adept at ignoring the signal distortion caused by ordinary room acoustics, but may be adapted to deal with its presence. These results are important for designing truly immersive spatial auditory displays, because they demonstrate the importance of reverberant energy for achieving a realistic, immersive experience.},
  langid = {english}
}

@online{singhImage2ReverbCrossModalReverb2021,
  title = {{{Image2Reverb}}: {{Cross-Modal Reverb Impulse Response Synthesis}}},
  shorttitle = {{{Image2Reverb}}},
  author = {Singh, Nikhil and Mentch, Jeff and Ng, Jerry and Beveridge, Matthew and Drori, Iddo},
  date = {2021-08-13},
  eprint = {2103.14201},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.14201},
  url = {http://arxiv.org/abs/2103.14201},
  urldate = {2025-07-19},
  abstract = {Measuring the acoustic characteristics of a space is often done by capturing its impulse response (IR), a representation of how a full-range stimulus sound excites it. This work generates an IR from a single image, which can then be applied to other signals using convolution, simulating the reverberant characteristics of the space shown in the image. Recording these IRs is both time-intensive and expensive, and often infeasible for inaccessible locations. We use an end-to-end neural network architecture to generate plausible audio impulse responses from single images of acoustic environments. We evaluate our method both by comparisons to ground truth data and by human expert evaluation. We demonstrate our approach by generating plausible impulse responses from diverse settings and formats including well known places, musical halls, rooms in paintings, images from animations and computer games, synthetic environments generated from text, panoramic images, and video conference backgrounds.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@online{Treble,
  title = {Treble},
  url = {https://www.treble.tech/},
  urldate = {2025-07-19}
}

@inproceedings{veauxVoiceBankCorpus2013,
  title = {The Voice Bank Corpus: {{Design}}, Collection and Data Analysis of a Large Regional Accent Speech Database},
  shorttitle = {The Voice Bank Corpus},
  booktitle = {2013 {{International Conference Oriental COCOSDA}} Held Jointly with 2013 {{Conference}} on {{Asian Spoken Language Research}} and {{Evaluation}} ({{O-COCOSDA}}/{{CASLRE}})},
  author = {Veaux, Christophe and Yamagishi, Junichi and King, Simon},
  date = {2013-11},
  pages = {1--4},
  doi = {10.1109/ICSDA.2013.6709856},
  url = {https://ieeexplore.ieee.org/document/6709856},
  urldate = {2025-07-19},
  abstract = {The University of Edinburgh has started the development of a new speech database, the Voice Bank corpus, specifically designed for the creation of personalised synthetic voices for individuals with speech disorders. This corpus already constitutes the largest corpora of British English currently in existence, with more than 300 hours of recordings from approximately 500 healthy speakers. New recordings are continuously being made in order to get the best coverage of the different combinations of regional accents, social classes, age and gender across Britain. This paper describes the motivation and the processes involved in the design and recording of this corpus as well as some analysis of its content. The paper concludes with our future plans to further extend this corpus and to overcome its current limitations.},
  eventtitle = {2013 {{International Conference Oriental COCOSDA}} Held Jointly with 2013 {{Conference}} on {{Asian Spoken Language Research}} and {{Evaluation}} ({{O-COCOSDA}}/{{CASLRE}})},
  keywords = {Corpus Design,Databases,Educational institutions,Hidden Markov models,Optimization,Recruitment,Speech,Speech synthesis,Speech Synthesis,Text Selection,Voice Banking}
}

@article{wittebolHybridRoomAcoustic2024,
  title = {A Hybrid Room Acoustic Modeling Approach Combining Image Source, Acoustic Diffusion Equation, and Time-Domain Discontinuous {{Galerkin}} Methods},
  author = {Wittebol, Wouter and Wang, Huiqing and Hornikx, Maarten and Calamia, Paul},
  date = {2024-07-05},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {223},
  pages = {110068},
  issn = {0003-682X},
  doi = {10.1016/j.apacoust.2024.110068},
  url = {https://www.sciencedirect.com/science/article/pii/S0003682X24002196},
  urldate = {2025-07-16},
  abstract = {In this paper a hybrid model is introduced that constructs a broadband room impulse response using a geometrical (image source method) and a statistical method (acoustic diffusion equation) for the high-frequency range, supported by a wave-based method (time-domain discontinuous Galerkin method) for the low-frequency range. A crucial element concerns the construction of the high-frequency impulse response where a transition from a predominantly specular (image source) to a predominantly diffuse sound-field (diffusion equation) is required. To achieve this transition an analytical envelope is introduced. A key factor is the room-averaged scattering coefficient which accounts for all scattering behavior of the room and determines the speed of transition from a specular to a non-specular sound-field. To evaluate its performance, the model is compared to a broadband wave-based solver for two reference scenarios. The hybrid model shows promising results in terms of reverberation time (T20), center time (Ts) and bass-ratio (BR). Aspects such as the used geometrical complexity, the ‘room-averaged’ scattering coefficients, and other model simplifications and assumptions are discussed.},
  keywords = {Acoustic diffusion equation method,Hybrid,Image source method,Room acoustics,Time-domain discontinuous Galerkin method}
}

@online{yangDepthAnythingUnleashing2024,
  title = {Depth {{Anything}}: {{Unleashing}} the {{Power}} of {{Large-Scale Unlabeled Data}}},
  shorttitle = {Depth {{Anything}}},
  author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  date = {2024-04-07},
  eprint = {2401.10891},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.10891},
  url = {http://arxiv.org/abs/2401.10891},
  urldate = {2025-07-19},
  abstract = {This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (\textasciitilde 62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@book{zotterAmbisonicsPractical3D2019,
  title = {Ambisonics: {{A Practical 3D Audio Theory}} for {{Recording}}, {{Studio Production}}, {{Sound Reinforcement}}, and {{Virtual Reality}}},
  shorttitle = {Ambisonics},
  author = {Zotter, Franz and Frank, Matthias},
  date = {2019},
  series = {Springer {{Topics}} in {{Signal Processing}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  issn = {1866-2609, 1866-2617},
  doi = {10.1007/978-3-030-17207-7},
  url = {http://link.springer.com/10.1007/978-3-030-17207-7},
  urldate = {2025-07-18},
  isbn = {978-3-030-17206-0 978-3-030-17207-7},
  langid = {english},
  keywords = {Ambisonics Book,Binaural Listening,Imaginary Loudspeakers,Open Access,Playback Technology,Psychoacoustics,Scene-based Sound,Spherical Array Processing,Surround Sound Recording}
}
